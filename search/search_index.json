{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>This site contains tutorials for mini-projects which are to be run on the Edinburgh Napier University Compute Cluster (ENUCC). These tutorials aim to be</p> <ul> <li>Small - the projects are toy-projects meant to illustrate a single concept.</li> <li>Self contained - the projects should be able to be run from start to finis, with no steps left implicit.</li> <li>Useful - by completing the tutorial you will develop skills which will be useful in a more complex piece of work.</li> </ul>"},{"location":"#want-a-tutorial-run-as-an-interactive-session","title":"Want a tutorial run as an interactive session?","text":"<p>If you want a tutorial to be run as an interactive session for multiple people, email l.headley@napier.ac.uk.</p>"},{"location":"#problems","title":"Problems","text":"<p>If you encounter any problems then please either raise an issue on the github page or contact l.headley@napier.ac.uk.</p>"},{"location":"useful-resources/","title":"Useful resources","text":"<ul> <li>Slurm docs - A useful place for learning more about how to use slurm.</li> <li>Tmux - Tmux is a useful tool for </li> <li>Introduction to Bash scripting</li> <li>Command line basics</li> <li>ShellCheck - A tool for checking the correctness of shell scripts.</li> <li>fish shell - If you are working a lot from the command line I highly recommend using the fish shell rather than bash. It is not currently available on ENUCC however.</li> <li>zsh - An alternative to bash and fish. Probably more widely adopted than fish (but in my opinion fish is better). Commonly used with OhMyZsh.</li> <li>WSL - The best way to use linux from windows. You will have access to a bash shell and the complete operating system. </li> </ul>"},{"location":"tutorials/01-hello-world/","title":"Hello World","text":""},{"location":"tutorials/01-hello-world/#description","title":"Description","text":"<p>In this project we will run a script which will write \"Hello world!\" to stdout. We look at running this script using <code>srun</code> and <code>sbatch</code>.</p>"},{"location":"tutorials/01-hello-world/#prerequisites","title":"Prerequisites","text":"<p>All you need to follow this tutorial is an account and a way of sshing to the server. If you do not yet have an account please request one. To ssh from windows, we recommend using PuTTY (available through AppsAnywhere).</p> <p>Code blocks in this document represent what you'll type into the terminal and also show you what you should see. Lines starting with a <code>$</code> are what you'll type (no need to type the <code>$</code>, it just represents the prompt) and lines which don't start with <code>$</code> are what you should see.</p> <p>Let's start by logging in to ENUCC. If you are using Linux, you can log in using the following command in a terminal:</p> <pre><code>$ ssh USERNAME@login.enucc.napier.ac.uk\n</code></pre> <p>If you are on windows, check out logging in with PuTTY.</p> <p>Organising your home directory is important. I recommend you make a projects directory which will contain all of your different projects. </p> <pre><code>$ mkdir projects\n$ cd projects\n</code></pre> <p>Then you can make a directory which will contain the code for this tutorial.</p> <pre><code>$ mkdir hello-world\n$ cd hello-world\n</code></pre> <p>To print text to stdout (which will make it visible in your terminal) we will be using the <code>echo</code> command. Let's try it out.</p> <pre><code>$ echo Hello world!\nHello world!\n</code></pre> <p>In order to make this more reusable, we'll put it in a bash script. A bash script is a file which contains a series of bash commands, just like you would type into the terminal. To create a new file using the command line:</p> <pre><code>$ touch hello_world.sh\n</code></pre> <p>To edit your file you'll need to use a text editor. You have several options for editing a file on ENUCC. You can use either nano, vim or emacs. nano is the easiest to get started with. Open a file with <code>nano filename</code>, edit it, using the arrow keys to navigate, \"ctrl + O\" to write the file (it will ask for the name, press enter to accept) and \"ctrl + X\" to exit the editor. Check out this cheetsheet for more info.</p> <p>Edit your <code>hello_world.sh</code> file and put the following into it:</p> <pre><code>#!/bin/bash\n\necho Hello world!\n</code></pre> <p>At this point you might be asking yourself why we've added an extra line at the top of the file. This is called a shebang interpreter directive and it is used to tell the operating system what interpreter to use to run the script. The shebang we've used says to use the bash shell, installed at <code>/bin/bash</code>. Don't worry if this is a bit confusing, just know that you need that line at the top of the script for it to run correctly.</p> <p>In order to execute the file, you'll need to make it executable. We edit the file permissions using the <code>chmod</code> command.</p> <pre><code>$ chmod u+x hello_world.sh\n</code></pre> <p>This makes it executable for the current user only (hence the u+x). Now let's run the file.</p> <pre><code>$ ./hello_world.sh\nHello world!\n</code></pre> <p>We've just run our first bit of code on ENUCC! Well done! But don't get too excited yet - we've only run it on the login node. ENUCC consists of several nodes which you can think of as separate computers with a very fast network connection between them. There are different types of node with different properties and here we'll talk about two such nodes: compute and login. There is one login node, which is used by all users as the entrypoint to the system. Login nodes however are quite minimally provisioned - they have only 4 cpus and 8gb of RAM. If lots of people were to run computationally intensive tasks on the login node it would lead to the system being unusable.</p> <p>Compute nodes on the other hand are built to deal with computationally intensive tasks. They have 64 cpus and 512gb of RAM. However you cannot use them directly like you used the login node. We use a program called slurm as our cluster management and job scheduling system. Slurm provides a program called <code>srun</code> which can be used to run your scripts on the compute nodes. Let's run this now.</p> <pre><code>$ srun --ntasks 1 --cpus-per-task 1 ./hello_world.sh\nsrun: job 27870 queued and waiting for resources\nsrun: job 27870 has been allocated resources\nHello world!\n</code></pre> <p>Here we've used the srun command with two flags. The first flag says we only have one task to run and the second indicates that we only want one cpu for this task. Specifying these flags means that other users will be able to run programs on the unused cpus on the same node. There are many more flags for srun which might be useful for your work. Check out the docs for more information.</p> <p>It's great that we can now run programs on the compute nodes, but what if we want to leave a job running for a long time and don't want to stay logged in while it's running? This is where the <code>sbatch</code> command comes in. <code>sbatch</code> lets you submit a job to ENUCC and slurm will run it when the resources become available. Once you submit a job with <code>sbatch</code> you do not need to stay logged in. A batch file is just like a regular bash file with some special directives. Let's edit <code>hello_world.sh</code> to convert it to a batch script.</p> <pre><code>#!/bin/bash\n#SBATCH --ntasks 1\n#SBATCH --cpus-per-task 1\n\necho Hello world!\n</code></pre> <p>Now all we need to do is use the <code>sbatch</code> command to submit it to the job queue.</p> <pre><code>$ sbatch hello_world.sh\nSubmitted batch job 27872\n</code></pre> <p>It's been submitted, but where is our output? Since we might not be logged in when it runs, slurm will not allow the job to print to our terminal. Instead it will redirect any output to a file. That file will be in the working directory from which you call the <code>sbatch</code> command. We can see our output by using the cat command on the output file, which by default is called <code>slurm-&lt;JOB_ID&gt;.out</code>.</p> <pre><code>$ cat slurm-27872.out\nHello world!\n</code></pre>"},{"location":"tutorials/01-hello-world/#how-is-this-useful","title":"How is this useful?","text":"<p>The process you used to run this simple script on the cluster is exactly the same as the process you'd use to run any script which would run on a single node. You can change the bash script to run python scripts, organise files or otherwise automate your computational workloads.</p> <p>Looking for a next step? Try running some scripts you'd normally run locally on ENUCC instead. Use a batch script to run them overnight.</p>"},{"location":"tutorials/02-running-python-analysis/","title":"Running Python Analysis","text":""},{"location":"tutorials/02-running-python-analysis/#description","title":"Description","text":"<p>This project covers how to move long-running scripts from running on your local machine to running on ENUCC. We will learn how to run a series of unconnected jobs in parallel using <code>sbatch</code> and <code>srun</code> and then we'll use the array flag to supply a range of values which will be parameters supplied to the script we're running.</p>"},{"location":"tutorials/02-running-python-analysis/#prerequisites","title":"Prerequisites","text":"<p>Ideally you should have run scripts using <code>srun</code> and <code>sbatch</code> before (if not check out the hello world project). </p>"},{"location":"tutorials/02-running-python-analysis/#initial-script","title":"Initial script","text":"<p>Let's say we have some python analysis that runs on a laptop, but takes some time to run. We want to run it 20 times with different parameters. Ideally we would set up a script to submit all 20 runs to ENUCC. We then come back in a few day and check our results.</p> <p>As a simple example, let's say we want to calculate some statistics about the product of the outcomes of two dice rolls. We also want to see how these statistics change depending on how many sides the dice have. Here is a simple script that takes the number of sides of the dice and the number of trials as command line arguments and prints the mean, mode and standard deviation of the product of the two dice rolls.</p> <pre><code>import sys\nimport random\nimport statistics\n\nn_sides = int(sys.argv[1])\nn_trials = int(sys.argv[2])\n\nfirst_die_outcomes = []\nsecond_die_outcomes = []\n\nfor i in range(n_trials):\nfirst_die_outcomes.append(random.randint(1, n_sides))\nsecond_die_outcomes.append(random.randint(1, n_sides))\n\nproducts = [a * b for a, b in zip(first_die_outcomes, second_die_outcomes)]\n\noutput = (f\"------------------------------------------------\\n\"\n      f\"Product of two {n_sides} sided dice. {n_trials} trials\\n\"\n      f\"------------------------------------------------\\n\"\n      f\"Mean: {statistics.mean(products)}\\n\"\n      f\"Median: {statistics.median(products)}\\n\"\n      f\"Mode: {statistics.mode(products)}\\n\")\n\nprint(output)\n</code></pre>"},{"location":"tutorials/02-running-python-analysis/#getting-files-onto-enucc","title":"Getting files onto ENUCC","text":"<p>The first thing we need to do is get the files from our local computer to enucc. Ideally you should be using git to manage your projects, with a remote repository on a hosting site such as github or gitlab. If you already have this setup it's a simple matter to clone the repository. I have put all of the code we need into a git repository so you should try cloning this now.</p> <pre><code>$ git clone git@github.com:SCEBE-Technicians/python-analysis-tutorial.git\n</code></pre> <p>If you aren't using git you can transfer files from either Linux or MacOS using either scp or rsync. The syntax is as follows:</p> <pre><code>$ scp /path/to/local/file 400XXXXX@login.enucc.napier.ac.uk:/path/to/server/file\n</code></pre> <p>or</p> <pre><code>$ rsync /path/to/local/file 400XXXXX@login.enucc.napier.ac.uk:/path/to/server/file\n</code></pre> <p>You can also use these programs to copy files from the server to your local machine. If you are on windows you can use scp from powershell.</p> <pre><code>&gt; scp \\path\\to\\local\\file 400XXXXX@login.enucc.napier.ac.uk:/path/to/server/file\n</code></pre>"},{"location":"tutorials/02-running-python-analysis/#where-should-i-put-data-files","title":"Where should I put data files?","text":"<p>This will be covered more thoroughly in a future project but the short answer is that you want to put large data files in <code>sharedscratch</code> for long term-storage or in <code>localscratch</code> for short-term access. <code>localscratch</code> is faster but will get wiped after 30 days.</p>"},{"location":"tutorials/02-running-python-analysis/#using-anaconda","title":"Using Anaconda","text":"<p>On ENUCC, python is managed with anaconda and so we have to set up our system to use anaconda. You simply need to load the correct module and initialise conda. You'll have to restart your shell (by logging out and in) after running these two commands.</p> <pre><code>$ module load apps/anaconda3\n$ conda init bash\n</code></pre> <p>To run the script for 1000 trials of 6 sided dice we simply run</p> <pre><code>$ python product_of_two_dice_analysis.py 6 1000\n------------------------------------------------\nProduct of two 10 sided dice. 10 trials\n------------------------------------------------\nMean: 29.4\nMedian: 23.5\nMode: 30\n</code></pre>"},{"location":"tutorials/02-running-python-analysis/#running-the-script","title":"Running the script","text":"<p>To run the same script on ENUCC we can use <code>srun</code>. Here we run it for a reasonably large number of trials.</p> <pre><code>$ srun python product_of_two_dice_analysis.py 10 10000000\nsrun: job 27947 queued and waiting for resources\nsrun: job 27947 has been allocated resources\n------------------------------------------------\nProduct of two 10 sided dice. 10000000 trials\n------------------------------------------------\nMean: 30.2445156\nMedian: 24.0\nMode: 6\n</code></pre> <p>So now we want to run it many times, for different number of sides and different numbers of trials. For this we'll have to write a batch script.</p> <pre><code>#!/bin/bash\n\nstart_time=`date +%s.%N`\n\npython product_of_two_dice_analysis.py 6 10000000\n\n\nend_time=`date +%s.%N`\nruntime=$(echo \"$end_time - $start_time\" | bc)\nprintf \"This job ran in %s seconds\\n\\n\" $runtime\n</code></pre> <p>Here I've used date and bc to calculate the runtime of the of the program. This will be important later when we're trying to guage how well our script will scale.</p> <p>As usual, we submit it with sbatch and see our output in the output file.</p> <pre><code>$ sbatch job_script.sh\nSubmitted batch job 27949\n\n$ cat slurm-27949.out\n------------------------------------------------\nProduct of two 6 sided dice. 1000 trials\n------------------------------------------------\nMean: 12.493\nMedian: 10.0\nMode: 6\n\nThis job ran in 11.822068605 seconds\n</code></pre> <p>Great. We're now in a position that we can modify our script to run our job a few times. Let's start by running 10 identical copies of it. Modify the bash script like so:</p> <pre><code>#!/bin/bash\n\nstart_time=`date +%s.%N`\n\npython product_of_two_dice_analysis.py 6 10000000\npython product_of_two_dice_analysis.py 6 10000000\npython product_of_two_dice_analysis.py 6 10000000\npython product_of_two_dice_analysis.py 6 10000000\npython product_of_two_dice_analysis.py 6 10000000\npython product_of_two_dice_analysis.py 6 10000000\npython product_of_two_dice_analysis.py 6 10000000\npython product_of_two_dice_analysis.py 6 10000000\npython product_of_two_dice_analysis.py 6 10000000\npython product_of_two_dice_analysis.py 6 10000000\n\nend_time=`date +%s.%N`\nruntime=$(echo \"$end_time - $start_time\" | bc)\nprintf \"This job ran in %s seconds\\n\\n\" $runtime\n</code></pre> <p>The output of the job script is quite long this time, so let's just see how long it ran for this time.</p> <pre><code>$ cat slurm-27951.out\n...\nThis job ran in 120.238923031 seconds\n</code></pre> <p>This has taken around 10 times longer than running the analysis once which is more or less what might be expected. ENUCC has a great deal of resources, but the script as we've written it requests only one CPU on one node and then runs the python script steps one after the other. There is no reason why we shouldn't run the python steps all at once, but we don't want to submit 10 different job. Instead we can run each python step as a separate task and assign one cpu to each task. We also have to specify how much memory each task should use since the default would request all available memory.</p>"},{"location":"tutorials/02-running-python-analysis/#parallelising-the-job","title":"Parallelising the job","text":"<pre><code>#!/bin/bash\n#SBATCH --ntasks 10\n#SBATCH --nodes 1\n#SBATCH --cpus-per-task 1\n\nstart_time=`date +%s.%N`\n\nsrun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\nsrun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\nsrun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\nsrun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\nsrun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\nsrun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\nsrun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\nsrun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\nsrun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\nsrun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\n\nwait\n\nend_time=`date +%s.%N`\nruntime=$(echo \"$end_time - $start_time\" | bc)\nprintf \"This job ran in %s seconds\\n\\n\" $runtime\n</code></pre> <p>To shorten the script and make it more readable we can also put our execution tasks in a loop.</p> <pre><code>#!/bin/bash\n#SBATCH --ntasks 10\n#SBATCH --nodes 1\n#SBATCH --cpus-per-task 1\n\nstart_time=`date +%s.%N`\n\nfor x in {1..10}\ndo\n  srun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\ndone\n\nwait\n\nend_time=`date +%s.%N`\nruntime=$(echo \"$end_time - $start_time\" | bc)\nprintf \"This job ran in %s seconds\\n\\n\" $runtime\n</code></pre> <p>Things are getting quite complicated now so let's take stock of what's happening. In the first few lines we specify that we'll be running 10 tasks on one node with one cpu per task. Instead of simply executing our code with <code>python script.py</code> now we use <code>srun</code> which is part of the slurm ecosystem. We use <code>srun</code> because we can specify what resources we want to use by passing in different flags. The <code>--exclusive</code> flag says that this job will have exclusive access to the requested resources. <code>--ntasks</code> says that this is only a single task. Then we supply the commands to run. Finally we add the <code>&amp;</code> character which runs the <code>srun</code> command in the background. This is necessary so that it can execute the next command before the current one has finished.</p> <p>When we run this script we can see the runtime is massively improved.</p> <pre><code>$ sbatch job_script.sh\n...\nThis job ran in 12.563785716 seconds \n</code></pre> <p>Note that as long as we have free resources on which to run the scripts, we can easily expand beyond running 10 copies and it will still take around 12 seconds. If we go beyond 64 tasks we will require more than one node.</p>"},{"location":"tutorials/02-running-python-analysis/#using-job-array","title":"Using job array","text":"<p>Now let's say we want to run this same script but change the number of sides each dice take, running it for 2, 4, 6 sides. Yes we could just use bash loops, but there is a simpler way to do this using the <code>--array</code> flag. Let's see what it looks like:</p> <pre><code>#!/bin/bash\n#SBATCH --ntasks 1\n#SBATCH --array=2,4,6\n#SBATCH --cpus-per-task 1\n#SBATCH --output=variable-sided-dice-result.out\n\npython product_of_two_dice_analysis.py $SLURM_ARRAY_TASK_ID 10000000 &amp;\n</code></pre> <p>This is much simpler than the previous script. Using the <code>--array</code> flag we specify what values we want to run the job for and we use <code>$SLURM_ARRAY_TASK_ID</code> to access those values in the script. When we submit this job it creates 3 versions of the job. We can see that the job id has an array appended with the specified values.</p> <pre><code>$ squeue\n         JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n 27995_[2,4,6]     nodes job_scri 40019142 PD       0:00      1 (Priority)\n</code></pre> <p>The <code>--array</code> flag is not limited to specifying individual values. There are a few different options for specifying values depending on your needs.</p> <code>--array</code> description 1,2,3 1,2,3 1-100 1,2 ... 100 1-100:2 1, 3, 5 ... 99 1-100%5 1,2 ... 100, running 5 jobs simultaneously"},{"location":"tutorials/02-running-python-analysis/#conclusion","title":"Conclusion","text":"<p>We've seen a few different ways that we can launch jobs. While launching multiple jobs with the array flag will likely be the most common usage, it is good to know how to launch individual jobs within a batch file so that we can have finer control over what scripts are running.</p>"},{"location":"tutorials/03-simple-neural-network/","title":"Simple Neural Network","text":""},{"location":"tutorials/03-simple-neural-network/#description","title":"Description","text":"<p>This is a barebones machine learning project. It involves getting some data, storing it in an appropriate location, installing packages with conda and running the training on ENUCC.</p> <p>This tutorial is largely based on a tutorial on machinelearningmastery.com. </p>"},{"location":"tutorials/03-simple-neural-network/#prerequisites","title":"Prerequisites","text":"<p>You should be familiar with writing and submitting jobs to ENUCC and also have some knowledge of python. This is not a machine learning tutorial, but meant to show you how to run a machine learning project on ENUCC.</p> <p>We start by getting the data by downloading it using the wget program.</p> <pre><code>$ mkdir ~/sharedscratch/diabetes-dataset\n$ cd ~/sharedscratch/diabetes-dataset\n$ wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\n</code></pre> <p>Note that I choose to store the data in the <code>sharedscratch</code> folder. This is a shared directory which does not have a storage quota. If the speed of loading data is a big factor for your work then you should use <code>localscratch</code> instead, but note that it is wiped at regular intervals and should not be used for long-term storage.</p> <p>We then create a folder to container our analysis</p> <pre><code>$ mkdir -p ~/projects/diabetes-analysis\n$ cd ~/projects/diabetes-analysis\n</code></pre> <p>In order to run the analysis we have to have python, numpy and tensorflow installed. We do this in a conda environment called <code>ml-example</code>. If you haven't used conda on ENUCC before then you'll have to set it up by running the following.</p> <pre><code>$ module load apps/anaconda3\n$ conda init bash\n</code></pre> <p>Once conda is configured, you can create the environment.</p> <pre><code>$ conda create -n ml-example\n$ conda activate ml-example\n$ conda install python=3.11 numpy tensorflow\n</code></pre> <p>To train the machine learning model we'll use the following script</p> <pre><code>from numpy import loadtxt\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom pathlib import Path\n\ndata_path = Path.home() / \"sharedscratch\" / \"diabetes-dataset\" / \"pima-indians-diabetes.data.csv\"\ndataset = loadtxt(data_path, delimiter=',')\nx = dataset[:, 0:8]\ny = dataset[:, 8]\n\nmodel = Sequential()\nmodel.add(Dense(12, input_shape=(8,), activation='relu'))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.fit(x, y, epochs=50, batch_size=10)\n\n_, accuracy = model.evaluate(x, y)\nprint('Accuracy: %.2f' % (accuracy*100))\n</code></pre> <p>The batch file to submit the job is quite straightforward</p> <pre><code>#!/bin/bash\n\npython model.py\n</code></pre> <p>And that's it! Sin \u00e9.</p>"},{"location":"tutorials/04-jupyter-notebook/","title":"Jupyter Notebook","text":""},{"location":"tutorials/04-jupyter-notebook/#description","title":"Description","text":"<p>Many people use jupyter notebooks for writing and running python code - especially when doing data science tasks. This tutorial will show you how to take a jupyter notebook which is running on your own computer and run it on ENUCC.</p>"},{"location":"tutorials/04-jupyter-notebook/#prerequisites","title":"Prerequisites","text":"<p>You should know the basics of creating and submitting jobs to ENUCC. You should also know a bit about creating and running jupyter notebooks.</p> <p>I have set up an example notebook which can be obtained from github. In a real usage scenario we might expect the notebook to do some heavy number crunchy, or neural network training, but here it simply generates a graph and puts it in the figures subdirectory. We're happy that the notebook works on the local machine, but it takes a long time and so we'd rather offload the effort to ENUCC. The first step is to clone the repository:</p> <pre><code>$ cd projects\n$ git clone git@github.com:SCEBE-Technicians/ENUCC-jupyter-notebook-example.git\n</code></pre> <p>If you've never used conda on ENUCC before then you'll need to do some setup. First load the conda module and then initialise it.</p> <pre><code>$ module load apps/anaconda3\n$ conda init bash\n</code></pre> <p>You'll have to restart the shell for these changes to take effect (the easiest way is to log out and back in).</p> <p>Now, in the repository that you cloned previously, there is a conda environment already defined. You can create a new conda environment from an <code>env.yml</code> file by running</p> <pre><code>$ cd ~/projects/ENUCC-jupyter-notebook-example\n$ conda env create -f env.yml\n$ conda activate jupyter-example\n</code></pre> <p>This environment has all the dependencies you'll need already installed, namely jupyter, numpy and matplotlib.</p> <p>We are now in a position to execute the notebook. This is really quite straightforward. If there is a free compute node (run <code>squeue</code> to check) then we can run it as follows.</p> <pre><code>$ srun jupyter execute NoisySine.ipynb\nsrun jupyter execute NoisySine.ipynb\n[NbClientApp] Executing NoisySine.ipynb\n...\n</code></pre> <p>You can then check in the <code>./figures</code> directory to see that a new figure has been added.</p> <p>As usual, <code>srun</code> is handy if there is a free compute node and your script won't take too long to run, but it's better to create a batch script if it is a long-running script, or if you expect it to be queued for a while.</p> <pre><code>#!/bin/bash\n#SBATCH --ntasks 1\n#SBATCH --nodes 1\n#SBATCH --cpus-per-task 1\n\njupyter execute NoisySine.ipynb\n</code></pre>"},{"location":"tutorials/04-jupyter-notebook/#configuring-conda","title":"Configuring conda","text":"<p>You have some options for using conda with ENUCC. If you find your environments are getting to be too big then you can move your modules to a location within shared scratch</p> <pre><code>$ conda config --add pkg_dirs sharedscratch/.conda/pkgs\n$ conda config --add envs_dirs sharedscratch/.conda/envs\n</code></pre> <p>You can also disable the base env being activated upon login.</p> <pre><code>$ conda config --set auto_activate_base false\n</code></pre>"},{"location":"tutorials/message-passing-programming/","title":"Message Passing Programming","text":"<p> This is a work in progress. </p>"},{"location":"tutorials/message-passing-programming/#description","title":"Description","text":"<p>Compute nodes on ENUCC each have up to 64 cores which have access to shared memory. When looking to run a job with more than 64 cores, we have to use multiple nodes, but separate nodes do not have shared memory. We need to use message passing programming to send information between nodes. We will be exploring the basics of message passing programming with MPI. The goal with this tutorial is to give you a taste of MPI programming along with an understanding of its capabilities so that you can understand whether it will be useful for future applications. This is not a comprehensive course.</p>"},{"location":"tutorials/message-passing-programming/#prerequisites","title":"Prerequisites","text":"<p>You should be comfortable using slurm to submit batch scripts. You should have a basic knowledge of the C programming language. Here is a quick reference in case you aren't used to programming in C. If have strong programming skills in a higher level programming language such as python you will probably still be able to follow along but some concepts in C such as pointers do not have an analogue in python.</p> <p>Message passing programming allows us to run a program with a very large number of cpus which do not necessarily have access to shared memory. Our program runs on several processes which can communicate with each other by passing messages. Let's start with a basic hello world program. Create a <code>hello_world.c</code> file with the following contents</p> <pre><code>#include &lt;stdio.h&gt;\n#include &lt;mpi.h&gt;\n\nint main (int argc, char **argv) {\n    MPI_Init(NULL, NULL);\n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank);\n\n    printf(\"Hello world from rank %d\\n\", world_rank);\n\n    return MPI_Finalize();\n}\n</code></pre> <p>in order to compile mpi programs we use the <code>mpicc</code> compile rather than <code>gcc</code>. You'll also have to load the mpi module.</p> <pre><code>$ module load mpi\n$ mpicc hello_world.c -o hello_world\n</code></pre> <p>We use the <code>mpirun</code> command to run it. As per usual, we need to use either <code>sbatch</code> or <code>srun</code> to run programs on the compute nodes. Here's a batch file which will run the compiled <code>hello_world</code> binary.</p> <pre><code>#!/bin/bash -l\n#SBATCH --export=ALL\n#SBATCH -o hello_world.output\n#SBATCH -J mpi-test\n#SBATCH --time=0-1\n#SBATCH --mem=1024\n#SBATCH --nodes=1\n#SBATCH --ntasks=4\n\n# module load mpi/openmpi\n\nmpirun hello_world\n</code></pre> <p>After submitting the script with <code>sbatch</code> we can inspect the output.</p> <pre><code>$ cat hello_world.output\nHello world from rank 0\nHello world from rank 1\nHello world from rank 2\nHello world from rank 3\n</code></pre> <p>We have run the task with four processes (by specifying <code>ntasks</code>) and it has printed a line on each of those processes.</p> <p>So now we have four processes running simultaneously, how can we get them to talk to each other? We send and receive messages!</p>"},{"location":"tutorials/message-passing-programming/#passing-a-message","title":"Passing a message","text":"<p>Let's write a very basic program which will send an integer from rank 0 to rank 1. Let's look at the program first and then we'll look at what's going on.</p> <pre><code>#include &lt;mpi.h&gt;\n#include &lt;stdio.h&gt;\n\nint main(int argc, char** argv) {\n    MPI_Init(NULL, NULL);\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);\n\n    if (rank == 0) {\n        int send = 5;\n        printf(\"Sending the number %d.\\n\", send);\n        MPI_Send(&amp;send, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);\n    } else if (rank == 1) {\n        int rec;\n        MPI_Recv(&amp;rec, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        printf(\"Received the number %d.\\n\", rec);\n    }\n    MPI_Finalize();\n}\n</code></pre> <p>Here we've used a few new methods, <code>MPI_Send</code> and <code>MPI_Recv</code>. All MPI methods typically follow a similar pattern. You can see the method definitions in the appendix. </p> <p>We call <code>MPI_Send</code> when we want to send data from one process to another. Then from the receiving process we need to call <code>MPI_Recv</code>. Since the same code will run for every process, we use <code>if</code> statements to send the message only on rank 0 and receive only on rank 1. There are some examples at the end of this tutorial which illustrate more interesting ways to use blocking communication.</p>"},{"location":"tutorials/message-passing-programming/#using-probe-to-get-the-message-size","title":"Using probe to get the message size","text":"<p>Let's say we want to send a string, but we don't know what size it's going to be. We can use <code>MPI_Probe</code> to get the size of the message.</p> <pre><code>#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n#include &lt;mpi.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;time.h&gt;\n\nint main(int argc, char** argv) {\n\n    MPI_Init(NULL, NULL);\n\n    // Initialise rank and world size\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);\n\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &amp;world_size);\n\n    // Need at least 2 processes\n    if (world_size &lt; 2) {\n        MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n\n        if (rank == 0) {\n        // Choose one of four animals\n        srand(time(NULL));\n        char animals[4][10] = {\"dog\", \"cat\", \"rat\", \"gorilla\"};\n        char animal[10];\n        int index = rand() % 4;\n        strcpy(animal, animals[index]);\n\n        printf(\"\\033[31mSending %s, of size %d\\n\", animal, strlen(animal));\n\n        MPI_Send(animal, strlen(animal), MPI_CHAR, 1, 0, MPI_COMM_WORLD);\n    } else if (rank == 1) {\n        // Get message status\n        MPI_Status stat;\n        MPI_Probe(0, 0, MPI_COMM_WORLD, &amp;stat);\n\n        // Get message length from status\n        int message_length;\n        MPI_Get_count(&amp;stat, MPI_CHAR, &amp;message_length);\n        printf(\"\\033[32mReceiving message of length %d\\n\", message_length);\n\n        // Receive message\n        char message[message_length];\n        MPI_Recv(&amp;message, message_length, MPI_CHAR, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        printf(\"Received %s from sender\\033[0m\\n\", message);\n    }\n\n    MPI_Finalize();\n}\n\n</code></pre>"},{"location":"tutorials/message-passing-programming/#broadcast","title":"Broadcast","text":"<p>The broadcast is one of the collective communication methods. The method is called on all processes. You provide the method a data buffer and specify the root process (by rank id). Broadcast will send a message to all other processes so that the variable is synchronised between processes. Here is an example where a random number is generated on process 0 and this is sent to all other processes.</p> <pre><code>#include &lt;mpi.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;time.h&gt;\n\nint main(int arg, char** argv) {\n    srand(time(NULL));\n\n    MPI_Init(NULL, NULL);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);\n    int i;\n    if (rank == 0) {\n            i = rand();\n        printf(\"sending %d to all processes\\n\", i);\n    }\n\n    MPI_Bcast(&amp;i, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    printf(\"%d\\n\", i);\n    MPI_Finalize();\n}\n</code></pre>"},{"location":"tutorials/message-passing-programming/#scatter-and-gather","title":"Scatter and Gather","text":""},{"location":"tutorials/message-passing-programming/#asynchronous-message-passing","title":"Asynchronous message passing","text":""},{"location":"tutorials/message-passing-programming/#examples","title":"Examples","text":""},{"location":"tutorials/message-passing-programming/#ping-pong","title":"Ping Pong","text":"<pre><code>#include &lt;mpi.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nint main(int argc, char** argv) {\n    const int PING_PONG_LIMIT = 10;\n\n    MPI_Init(NULL, NULL);\n\n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank);\n\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &amp;world_size);\n\n    if (world_size != 2) {\n        MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n\n    int ping_pong_count = 0;\n    int partner_rank = (world_rank + 1) % 2;\n\n    while(ping_pong_count &lt; PING_PONG_LIMIT) {\n        if (world_rank == ping_pong_count % 2) {\n            ping_pong_count++;\n\n            MPI_Send(&amp;ping_pong_count, 1, MPI_INT, partner_rank, 0, MPI_COMM_WORLD);\n            printf(\"%d sent from %d to %d\\n\", ping_pong_count, world_rank, partner_rank);\n        } else {\n            MPI_Recv(&amp;ping_pong_count, 1, MPI_INT, partner_rank, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            printf(\"%d received from %d on %d\\n\", ping_pong_count, world_rank, partner_rank);\n        }\n    }\n    MPI_Finalize();\n}\n</code></pre> <p>Challenge: implement a ring-pong program where instead of two processes, you have an arbitrary number of processes which pass a value around in a ring.</p>"},{"location":"tutorials/message-passing-programming/#1d-ising-model","title":"1D Ising Model","text":""},{"location":"tutorials/message-passing-programming/#next-steps","title":"Next steps","text":""},{"location":"tutorials/message-passing-programming/#useful-resources","title":"Useful resources","text":"<ul> <li>MPI Tutorial - if you are looking to get into message passing programming then doing this course in its entirety is almost certainly the best place to start.</li> <li>Microsoft MPI Reference - a nice reference guide.</li> </ul>"},{"location":"tutorials/message-passing-programming/#appendix","title":"Appendix","text":""},{"location":"tutorials/message-passing-programming/#method-definitions","title":"Method definitions","text":"<pre><code>MPI_Send(\n    void *buf, \n    int count, \n    MPI_Datatype datatype,\n    int dest, \n    int tag, \n    MPI_Comm comm\n);\n</code></pre> <ul> <li><code>buf</code> - a pointer to the data to be sent.</li> <li><code>count</code> - the number of elements in the buffer. If sending an array, it would be the length of the array.</li> <li><code>datatype</code> - the type of the data being sent. Must be one of the predefined datatypes.</li> <li><code>dest</code> - the rank of the receiver.</li> <li><code>tag</code> - a tag for distinguishing message types - set to 0 for most normal usage.</li> <li><code>comm</code> - the mpi communicator. Set to <code>MPI_COMM_WORLD</code> for most normal usage.</li> </ul> <pre><code>MPI_Recv(\n    void *buf,\n    int count,\n    MPI_Datatype datatype,\n    int source,\n    int tag,\n    MPI_Comm comm,\n    MPI_Status *status\n);\n</code></pre> <ul> <li><code>buf</code> - a pointer to the location where the received data will be stored.</li> <li><code>count</code> - the number of elements in the buffer. If sending an array, it would be the length of the array.</li> <li><code>datatype</code> - the type of the data being sent. Must be one of the predefined datatypes.</li> <li><code>source</code> - the rank of the sender.</li> <li><code>tag</code> - a tag for distinguishing message types - set to 0 for most normal usage.</li> <li><code>comm</code> - the mpi communicator. Set to <code>MPI_COMM_WORLD</code> for most normal usage.</li> <li><code>status</code> - contains a pointer to an MPI_Status.</li> </ul>"},{"location":"useful-extras/git/","title":"Using git","text":""},{"location":"useful-extras/git/#description","title":"Description","text":"<p>This tutorial will outline the basics of using git.</p>"},{"location":"useful-extras/git/#prerequisites","title":"Prerequisites","text":"<p>None.</p> <p>Git is a general purpose tool for tracking changes in a filesystem and for collaboratively working with others. There are a few ways to use git, some more complicated than others. In this tutorial we'll first look at using git for a project on which you are the only person making changes. In this scenario git is useful for tracking changes and also for keeping a remote copy of your work as a backup. After we become comfortable working alone with git, we'll look at using git to collaborate with others.</p> <p>Before we get started I want to cover a few bits of terminology which are frequently used.</p> <ul> <li>git: git is the name of the tool used for version control.</li> <li>Repository: all of your project files, along with your git history and data.</li> <li>GitHub: a website which allows you to host a copy of your repository on their servers.</li> <li>Commit: a snapshot of your project files</li> </ul> <p>To get started you can follow the instructions on github to create a new repository. Once you've created a repository, which is stored on github's servers, you can clone the repository to have a copy on your own machine.</p> <pre><code>$ git clone git@github.com:SCEBE-Technicians/&lt;repository-name&gt;.git\n</code></pre> <p>You can then make changes or create new files in the repository. Once you are happy with the changes made, we will want to make a commit. You can think of a commit as a snapshot of your project. Committing your changes happens in two parts - first you add the changed files you want to be included in the commit to the staging area, then you create the commit from the files in the staging area.</p> <pre><code>$ git add file1 file2 file3\n$ git commit -m \"Create file2 and file3 and change file1\"\n</code></pre> <p>The changes will not be visible on the remote repository (on github) yet, because we've only made the changes locally. In order to sync the remote repository with our local repository, we have to push the changes. </p> <pre><code>$ git push\n</code></pre>"},{"location":"useful-extras/git/#working-with-others","title":"Working with others","text":"<p>So far, everything we've done has been on the main branch.</p>"},{"location":"useful-extras/git/#useful-resources","title":"Useful resources","text":"<ul> <li>If you are looking to get deep into what git can really do and how it works then I'd recommend reading pro git. It's freely available online and is one of the best resources there is.</li> </ul>"},{"location":"useful-extras/logging-in-with-putty/","title":"Logging in with PuTTY","text":"<p>To log in with putty, first install it via apps anywhere. When you launch the application it should look like so:</p> <p></p> <p>Into the input box labelled Host Name (or IP address) type <code>4XXXXXXX@login.enucc.napier.ac.uk</code>, replacing the X's with your staff or student number.</p> <p></p> <p>The first time you do this, you'll be presented with the following warning. Press accept.</p> <p></p> <p>You'll then be prompted to type in your password. Nothing will show on screen as you type, this is normal.</p> <p></p> <p>Press enter, and you're now connected to a bash shell on the login node.</p> <p></p>"}]}
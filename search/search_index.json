{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>This site contains tutorials for mini-projects which are to be run on the Edinburgh Napier University Compute Cluster (ENUCC). These tutorials aim to be</p> <ul> <li>Small - the projects are toy-projects meant to illustrate a single concept.</li> <li>Self contained - the projects should be able to be run from start to finis, with no steps left implicit.</li> <li>Useful - by completing the tutorial you will develop skills which will be useful in a more complex piece of work.</li> </ul>"},{"location":"#want-a-tutorial-run-as-an-interactive-session","title":"Want a tutorial run as an interactive session?","text":"<p>If you want a tutorial to be run as an interactive session for multiple people, email l.headley@napier.ac.uk.</p>"},{"location":"#problems","title":"Problems","text":"<p>If you encounter any problems then please either raise an issue on the github page or contact l.headley@napier.ac.uk.</p>"},{"location":"tutorials/hello-world/","title":"Hello World","text":""},{"location":"tutorials/hello-world/#description","title":"Description","text":"<p>In this project we will run a script which will write \"Hello world!\" to stdout. We look at running this script using <code>srun</code> and <code>sbatch</code>.</p>"},{"location":"tutorials/hello-world/#prerequisites","title":"Prerequisites","text":"<p>All you need to follow is this tutorial is an account and a way of sshing to the server. If you do not yet have an account please request one. To ssh from windows, we recommend using PuTTY (available through AppsAnywhere).</p> <p>Code blocks in this document represent what you'll type into the terminal and show you what you should see. Lines starting with a <code>$</code> are what you'll type (no need to type the <code>$</code>, it just represents the prompt) and lines which don't start with <code>$</code> are what you should see.</p> <p>Let's start by logging in to ENUCC.</p> <pre><code>$ ssh USERNAME@login.enucc.napier.ac.uk\n</code></pre> <p>Organising your home directory is important. I recommend you make a projects directory which will contain all of your projects. </p> <pre><code>$ mkdir projects\n$ cd projects\n</code></pre> <p>Then you can make a directory which will contain the code for this tutorial.</p> <pre><code>$ mkdir hello-world\n$ cd hello-world\n</code></pre> <p>To print text to stdout (which will make it visible in your terminal) we will be using the <code>echo</code> command. Let's try it out.</p> <pre><code>$ echo Hello world!\nHello world!\n</code></pre> <p>In order to make this more reproducible, we'll put it in a bash script. A bash script is a file which contains a series of bash commands, just like you would type into the terminal. To create a new file using the command line:</p> <pre><code>$ touch hello_world.sh\n</code></pre> <p>To edit your file you'll need to use a text editor. You have several options for editing a file on ENUCC. You can use either nano, vim or emacs. nano is the easiest to get started with. Open a file with <code>nano filename</code>, edit it, using the arrow keys to navigate, \"ctrl + O\" to write the file (it will ask for the name, press enter to accept) and \"ctrl + X\" to exit the editor. Check out this cheetsheet for more info.</p> <p>Edit your <code>hello_world.sh</code> file and put the following into it:</p> <pre><code>#!/bin/bash\n\necho Hello world!\n</code></pre> <p>At this point you might be asking yourself why we've added an extra line at the top of the file. This is called a shebang interpreter directive and it is used to tell the operating system what interpreter to use to run the script. The shebang we've used says to use the bash shell, installed at <code>/bin/bash</code>. Don't worry if this is a bit confusing, just know that you need that line at the top of the script for it to run correctly.</p> <p>In order to execute the file, you'll need to make it executable. We edit the file permissions using the <code>chmod</code> command.</p> <pre><code>$ chmod u+x hello_world.sh\n</code></pre> <p>This makes it executable for the current user only (hence the u+x). Now let's run the file.</p> <pre><code>$ ./hello_world.sh\nHello world!\n</code></pre> <p>We've just run our first bit of code on ENUCC! Well done! But don't get too excited yet - we've only run it on the login node. ENUCC consists of several nodes which you can think of as separate computers with a very fast network connection between them. There are different types of node with different properties and here we'll talk about two such nodes: compute and login. There is one login node, which is used by all users as the entrypoint to the system. Login nodes however are quite minimally provisioned - they have only 4 cpus and 8gb of RAM. If lots of people were to run computationally intensive tasks on the login node it would lead to the system being unusable.</p> <p>Compute nodes on the other hand are built to deal with computationally intensive tasks. They have 64 cpus and 512gb of RAM. However you cannot use them directly like you used the login node. We use a program called slurm as our cluster management and job scheduling system. Slurm provides a program called <code>srun</code> which can be used to run your scripts on the compute nodes. Let's run this now.</p> <pre><code>$ srun --ntasks 1 --cpus-per-task 1 ./hello_world.sh\nsrun: job 27870 queued and waiting for resources\nsrun: job 27870 has been allocated resources\nHello world!\n</code></pre> <p>Here we've used the srun command with two flags. The first flag says we only have one task to run and the second indicates that we only want one cpu for this task. Specifying these flags means that other users will be able to run programs on the unused cpus on the same node. There are many more flags for srun which might be useful for your work. Check out the docs for more information.</p> <p>It's great that we can now run programs on the compute nodes, but what if we want to leave a job running for a long time and don't want to stay logged in while it's running? This is where the <code>sbatch</code> command comes in. <code>sbatch</code> lets you write submit a job to ENUCC and slurm will run it when the resources become available. Once you submit a job with <code>sbatch</code> you do not need to stay logged in. A batch file is just like a regular bash file with some special shebang directives. Let's edit <code>hello_world.sh</code> to convert it to a batch script.</p> <pre><code>#!/bin/bash\n#SBATCH --ntasks 1\n#SBATCH --cpus-per-task 1\n\necho Hello world!\n</code></pre> <p>Now all we need to do is use the <code>sbatch</code> command to submit it to the job queue.</p> <pre><code>$ sbatch hello_world.sh\nSubmitted batch job 27872\n</code></pre> <p>It's been submitted, but where is our output? Since we might not be logged in when it runs, slurm will not allow the job to print to our terminal. Instead it will redirect any output to a file. That file will be in the working directory from which you call the <code>sbatch</code> command. We can see our output by using the cat command on the output file, which by default is called <code>slurm-&lt;JOB_ID&gt;.out</code>.</p> <pre><code>$ cat slurm-27872.out\nHello world!\n</code></pre>"},{"location":"tutorials/hello-world/#how-is-this-useful","title":"How is this useful?","text":"<p>The process you used to run this simple script on the cluster is exactly the same as the process you'd use to run any script which would run on a single node. You can change the bash script to run python scripts, organise files or otherwise automate your computational workloads.</p> <p>Looking for a next step? Try running some scripts you'd normally run locally on ENUCC instead. Use a batch script to run them overnight.</p>"},{"location":"tutorials/message-passing-programming/","title":"Message Passing Programming","text":""},{"location":"tutorials/message-passing-programming/#description","title":"Description","text":"<p>Compute nodes on ENUCC each have 64 cores which have access to shared memory. When looking to run a job with more than 64 cores, we have to use multiple nodes, but separate nodes do not have shared memory. We need to use message passing programming to send information between nodes. We will be exploring the basics of message passing programming with MPI. The goal with this tutorial is to give you a taste of MPI programming along with an understanding of its capabilities so that you can understand whether it will be useful for future applications. This is not a comprehensive course.</p>"},{"location":"tutorials/message-passing-programming/#prerequisites","title":"Prerequisites","text":"<p>You should be comfortable using slurm to submit batch scripts. You should have a basic knowledge of the C programming language. Here is a quick reference in case you aren't used to programming in C. If have strong programming skills in a higher level programming language such as python you will probably still be able to follow along but some concepts in C such as pointers do not have an analogue in python.</p> <p>Message passing programming allows us to run a program with a very large number of cpus which do not necessarily have access to shared memory. Our program runs on several processes which can communicate with each other by passing messages. Let's start with a basic hello world program. Create a <code>hello_world.c</code> file with the following contents</p> <pre><code>#include &lt;stdio&gt;\n#include &lt;mpi.h&gt;\n\nint main (int argc, char **argv) {\n    int ierr = MPI_Init(&amp;argc, &amp;argv);\n    printf(\"Hello world\\n\");\n\n    return MPI_Finalize();\n}\n</code></pre>"},{"location":"tutorials/message-passing-programming/#ping-pong","title":"Ping Pong","text":"<pre><code>#include &lt;mpi.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nint main(int argc, char** argv) {\n    const int PING_PONG_LIMIT = 10;\n\n    MPI_Init(NULL, NULL);\n\n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank);\n\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &amp;world_size);\n\n    if (world_size != 2) {\n        MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n\n    int ping_pong_count = 0;\n    int partner_rank = (world_rank + 1) % 2;\n\n    while(ping_pong_count &lt; PING_PONG_LIMIT) {\n        if (world_rank == ping_pong_count % 2) {\n            ping_pong_count++;\n\n            MPI_Send(&amp;ping_pong_count, 1, MPI_INT, partner_rank, 0, MPI_COMM_WORLD);\n            printf(\"%d sent from %d to %d\\n\", ping_pong_count, world_rank, partner_rank);\n        } else {\n            MPI_Recv(&amp;ping_pong_count, 1, MPI_INT, partner_rank, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            printf(\"%d received from %d on %d\\n\", ping_pong_count, world_rank, partner_rank);\n        }\n    }\n    MPI_Finalize();\n}\n</code></pre> <p>Challenge: implement a ring-pong program where instead of two processes, you have an arbitrary number of processes which pass a value around in a ring.</p>"},{"location":"tutorials/message-passing-programming/#1d-ising-model","title":"1D Ising Model","text":""},{"location":"tutorials/message-passing-programming/#next-steps","title":"Next steps","text":""},{"location":"tutorials/message-passing-programming/#useful-resources","title":"Useful resources","text":"<ul> <li>MPI Tutorial</li> <li>Intro to MPI</li> <li>Introduction to slurm and mpi</li> </ul>"},{"location":"tutorials/running-python-analysis/","title":"Running Python Analysis","text":""},{"location":"tutorials/running-python-analysis/#description","title":"Description","text":"<p>This project covers how to move long-running scripts from running on your local machine to running on ENUCC. We will learn how to run a series of unconnected jobs in parallel using <code>sbatch</code> and <code>srun</code> and then we'll use the array command to supply a range of values which will be parameters supplied to the script we're running.</p>"},{"location":"tutorials/running-python-analysis/#prerequisites","title":"Prerequisites","text":"<p>Ideally you should have run scripts using <code>srun</code> and <code>sbatch</code> before (if not check out the hello world project). </p> <p>Let's say we have some python analysis that runs on a laptop, but takes some time to run. We want to run it 20 times with different parameters. Ideally we would set up a script to submit all 20 runs to ENUCC. We then come back in a few day and check our results. We don't need to sit and watch it and we can even configure slurm to email us when the job is finished.</p> <p>As a simple example, let's say we want to calculate some statistics about the product of the outcomes of two dice rolls. We also want to see how these statistics change depending on how many sides the dice have. Here is a simple script that takes the number of sides of the dice and the number of trials as command line arguments and prints the mean, mode and standard deviation of the product of the two dice rolls.</p> <pre><code>import sys\nimport random\nimport statistics\n\nn_sides = int(sys.argv[1])\nn_trials = int(sys.argv[2])\n\nfirst_die_outcomes = []\nsecond_die_outcomes = []\n\nfor i in range(n_trials):\nfirst_die_outcomes.append(random.randint(1, n_sides))\nsecond_die_outcomes.append(random.randint(1, n_sides))\n\nproducts = [a * b for a, b in zip(first_die_outcomes, second_die_outcomes)]\n\noutput = (f\"------------------------------------------------\\n\"\n      f\"Product of two {n_sides} sided dice. {n_trials} trials\\n\"\n      f\"------------------------------------------------\\n\"\n      f\"Mean: {statistics.mean(products)}\\n\"\n      f\"Median: {statistics.median(products)}\\n\"\n      f\"Mode: {statistics.mode(products)}\\n\")\n\nprint(output)\n</code></pre> <p>The first thing we need to do is get the files from our local computer to enucc. Ideally you should be using git to manage your projects, with a remote repository on a hosting site such as github or gitlab. If you already have this setup it's a simple matter to clone the repository. I have put all of the code we need into a git repository so you should try cloning this now.</p> <pre><code>$ git clone git@github.com:SCEBE-Technicians/python-analysis-tutorial.git\n</code></pre> <p>On ENUCC, python is managed with anaconda and so we have to first load the anaconda module</p> <pre><code>$ module load apps/anaconda3\n</code></pre> <p>If this is the first time you've tried to use anaconda it will ask you to run a setup script.</p> <pre><code>TODO: FIND OUT WHAT GOES HERE\n</code></pre> <p>To run the script for 1000 trials of 6 sided dice we simply run</p> <pre><code>$ python product_of_two_dice_analysis.py 6 1000\n------------------------------------------------\nProduct of two 10 sided dice. 10 trials\n------------------------------------------------\nMean: 29.4\nMedian: 23.5\nMode: 30\n</code></pre> <p>To run the same script on ENUCC we can use <code>srun</code>. Here we run it for a reasonably large number of trials.</p> <pre><code>$ srun python product_of_two_dice_analysis.py 10 10000000\nsrun: job 27947 queued and waiting for resources\nsrun: job 27947 has been allocated resources\n------------------------------------------------\nProduct of two 10 sided dice. 10000000 trials\n------------------------------------------------\nMean: 30.2445156\nMedian: 24.0\nMode: 6\n</code></pre> <p>So now we want to run it many times, for different number of sides and different numbers of trials. For this we'll have to write a batch script.</p> <pre><code>#!/bin/bash\n\nstart_time=`date +%s.%N`\n\npython product_of_two_dice_analysis.py 6 10000000\n\n\nend_time=`date +%s.%N`\nruntime=$(echo \"$end_time - $start_time\" | bc)\nprintf \"This job ran in %s seconds\\n\\n\" $runtime\n</code></pre> <p>Here I've used date and bc to calculate the runtime of the of the program. This will be important later when we're trying to guage how well our script will scale.</p> <p>As usual, we submit it with sbatch and see our output in the output file.</p> <pre><code>$ sbatch job_script.sh\nSubmitted batch job 27949\n\n$ cat slurm-27949.out\n------------------------------------------------\nProduct of two 6 sided dice. 1000 trials\n------------------------------------------------\nMean: 12.493\nMedian: 10.0\nMode: 6\n\nThis job ran in 11.822068605 seconds\n</code></pre> <p>Great. We're now in a position that we can modify our script to run our job a few times. Let's start by running 10 identical copies of it. Modify the bash script like so:</p> <pre><code>#!/bin/bash\n\nstart_time=`date +%s.%N`\n\npython product_of_two_dice_analysis.py 6 10000000\npython product_of_two_dice_analysis.py 6 10000000\npython product_of_two_dice_analysis.py 6 10000000\npython product_of_two_dice_analysis.py 6 10000000\npython product_of_two_dice_analysis.py 6 10000000\npython product_of_two_dice_analysis.py 6 10000000\npython product_of_two_dice_analysis.py 6 10000000\npython product_of_two_dice_analysis.py 6 10000000\npython product_of_two_dice_analysis.py 6 10000000\npython product_of_two_dice_analysis.py 6 10000000\n\nend_time=`date +%s.%N`\nruntime=$(echo \"$end_time - $start_time\" | bc)\nprintf \"This job ran in %s seconds\\n\\n\" $runtime\n</code></pre> <p>The output of the job script is quite long this time, so let's just see how long it ran for this time.</p> <pre><code>$ cat slurm-27951.out\n...\nThis job ran in 120.238923031 seconds\n</code></pre> <p>This has taken around 10 times longer than running the analysis once which is more or less what might be expected. ENUCC has a great deal of resources, but the script as we've written it requests only one CPU on one node and then runs the python script steps one after the other. There is no reason why we shouldn't run the python steps all at once, but we don't want to submit 10 different job. Instead we can run each python step as a separate task and assign one cpu to each task. We also have to specify how much memory each task should use since the default would request all available memory.</p> <pre><code>#!/bin/bash\n#SBATCH --ntasks 10\n#SBATCH --nodes 1\n#SBATCH --cpus-per-task 1\n\nstart_time=`date +%s.%N`\n\nsrun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\nsrun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\nsrun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\nsrun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\nsrun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\nsrun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\nsrun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\nsrun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\nsrun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\nsrun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\n\nwait\n\nend_time=`date +%s.%N`\nruntime=$(echo \"$end_time - $start_time\" | bc)\nprintf \"This job ran in %s seconds\\n\\n\" $runtime\n</code></pre> <p>To shorten the script and make it more readable we can also put our execution tasks in a loop.</p> <pre><code>#!/bin/bash\n#SBATCH --ntasks 10\n#SBATCH --nodes 1\n#SBATCH --cpus-per-task 1\n\nstart_time=`date +%s.%N`\n\nfor x in {1..10}\ndo\n  srun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\ndone\n\nwait\n\nend_time=`date +%s.%N`\nruntime=$(echo \"$end_time - $start_time\" | bc)\nprintf \"This job ran in %s seconds\\n\\n\" $runtime\n</code></pre> <p>Things are getting quite complicated now so let's take stock of what's happening. In the first few lines we specify that we'll be running 10 tasks on one node with one cpu per task. Instead of simply executing our code with <code>python script.py</code> now we use <code>srun</code> which is part of the slurm ecosystem. We use <code>srun</code> because we can specify what resources we want to use by passing in different flags. The <code>--exclusive</code> flag says that this job will have exclusive access to the requested resources. <code>--ntasks</code> says that this is only a single task. Then we supply the commands to run. Finally we add the <code>&amp;</code> character which runs the <code>srun</code> command in the background. This is necessary so that it can execute the next command before the current one has finished.</p> <p>When we run this script we can see the runtime is massively improved.</p> <pre><code>$ sbatch job_script.sh\n...\nThis job ran in 12.563785716 seconds \n</code></pre> <p>Note that as long as we have free resources on which to run the scripts, we can easily expand beyond running 10 copies and it will still take around 12 seconds. If we go beyond 64 tasks we will require more than one node.</p> <p>Now let's say we want to run this same script but change the number of sides each dice take, running it for 2, 4, 6 sides. Yes we could just use bash loops, but there is a simpler way to do this using the <code>--array</code> flag. Let's see what it looks like:</p> <pre><code>#!/bin/bash\n#SBATCH --ntasks 1\n#SBATCH --array=2,4,6\n#SBATCH --cpus-per-task 1\n#SBATCH --output=variable-sided-dice-result.out\n\npython product_of_two_dice_analysis.py $SLURM_ARRAY_TASK_ID 10000000 &amp;\n</code></pre> <p>This is much simpler than the previous script. Using the <code>--array</code> flag we specify what values we want to run the job for and we use <code>$SLURM_ARRAY_TASK_ID</code> to access those values in the script. When we submit this job it creates 3 versions of the job. We can see that the job id has an array appended with the specified values.</p> <pre><code>$ squeue\n         JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n 27995_[2,4,6]     nodes job_scri 40019142 PD       0:00      1 (Priority)\n</code></pre>"}]}
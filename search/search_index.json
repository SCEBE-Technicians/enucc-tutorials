{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>This site contains tutorials for mini-projects which are to be run on the Edinburgh Napier University Compute Cluster (ENUCC). These tutorials aim to be</p> <ul> <li>Small - the projects are toy-projects meant to illustrate a single concept.</li> <li>Self contained - the projects should be able to be run from start to finis, with no steps left implicit.</li> <li>Useful - by completing the tutorial you will develop skills which will be useful in a more complex piece of work.</li> </ul>"},{"location":"#want-a-tutorial-run-as-an-interactive-session","title":"Want a tutorial run as an interactive session?","text":"<p>If you want a tutorial to be run as an interactive session for multiple people, email l.headley@napier.ac.uk.</p>"},{"location":"#problems","title":"Problems","text":"<p>If you encounter any problems then please either raise an issue on the github page or contact l.headley@napier.ac.uk.</p>"},{"location":"useful-resources/","title":"Useful resources","text":"<ul> <li>Slurm docs - A useful place for learning more about how to use slurm.</li> <li>Tmux - Tmux is a useful tool for managing multiple terminal sessions. It can also be left running so when you exit the ssh session, you can keep processes running and resume where you left off when you next log in.</li> <li>Introduction to Bash scripting</li> <li>Command line basics</li> <li>ShellCheck - A tool for checking the correctness of shell scripts.</li> <li>fish shell - If you are working a lot from the command line I highly recommend using the fish shell rather than bash. It is not currently available on ENUCC however.</li> <li>zsh - An alternative to bash and fish. Probably more widely adopted than fish (but in my opinion fish is better). Commonly used with OhMyZsh.</li> <li>WSL - The best way to use linux from windows. You will have access to a bash shell and the complete operating system. </li> </ul>"},{"location":"tutorials/01-hello-world/","title":"Hello World","text":""},{"location":"tutorials/01-hello-world/#description","title":"Description","text":"<p>In this project we will run a script which will write \"Hello world!\" to stdout. We look at running this script using <code>srun</code> and <code>sbatch</code>.</p>"},{"location":"tutorials/01-hello-world/#prerequisites","title":"Prerequisites","text":"<p>All you need to follow this tutorial is an account and a way of sshing to the server. If you do not yet have an account please request one. To ssh from windows, we recommend using PuTTY (available through AppsAnywhere).</p> <p>Code blocks in this document represent what you'll type into the terminal and also show you what you should see. Lines starting with a <code>$</code> are what you'll type (no need to type the <code>$</code>, it just represents the prompt) and lines which don't start with <code>$</code> are what you should see.</p> <p>Let's start by logging in to ENUCC. If you are using Linux, you can log in using the following command in a terminal:</p> <pre><code>$ ssh USERNAME@login.enucc.napier.ac.uk\n</code></pre> <p>If you are on windows, check out logging in with PuTTY.</p> <p>Organising your home directory is important. I recommend you make a projects directory which will contain all of your different projects. </p> <pre><code>$ mkdir projects\n$ cd projects\n</code></pre> <p>Then you can make a directory which will contain the code for this tutorial.</p> <pre><code>$ mkdir hello-world\n$ cd hello-world\n</code></pre> <p>To print text to stdout (which will make it visible in your terminal) we will be using the <code>echo</code> command. Let's try it out.</p> <pre><code>$ echo Hello world!\nHello world!\n</code></pre> <p>In order to make this more reusable, we'll put it in a bash script. A bash script is a file which contains a series of bash commands, just like you would type into the terminal. To create a new file using the command line:</p> <pre><code>$ touch hello_world.sh\n</code></pre> <p>To edit your file you'll need to use a text editor. You have several options for editing a file on ENUCC. You can use either nano, vim or emacs. nano is the easiest to get started with. Open a file with <code>nano filename</code>, edit it, using the arrow keys to navigate, \"ctrl + O\" to write the file (it will ask for the name, press enter to accept) and \"ctrl + X\" to exit the editor. Check out this cheetsheet for more info.</p> <p>Edit your <code>hello_world.sh</code> file and put the following into it:</p> <pre><code>#!/bin/bash\n\necho Hello world!\n</code></pre> <p>At this point you might be asking yourself why we've added an extra line at the top of the file. This is called a shebang interpreter directive and it is used to tell the operating system what interpreter to use to run the script. The shebang we've used says to use the bash shell, installed at <code>/bin/bash</code>. Don't worry if this is a bit confusing, just know that you need that line at the top of the script for it to run correctly.</p> <p>In order to execute the file, you'll need to make it executable. We edit the file permissions using the <code>chmod</code> command.</p> <pre><code>$ chmod u+x hello_world.sh\n</code></pre> <p>This makes it executable for the current user only (hence the u+x). Now let's run the file.</p> <pre><code>$ ./hello_world.sh\nHello world!\n</code></pre> <p>We've just run our first bit of code on ENUCC! Well done! But don't get too excited yet - we've only run it on the login node. ENUCC consists of several nodes which you can think of as separate computers with a very fast network connection between them. There are different types of node with different properties and here we'll talk about two such nodes: compute and login. There is one login node, which is used by all users as the entrypoint to the system. Login nodes however are quite minimally provisioned - they have only 4 cpus and 8gb of RAM. If lots of people were to run computationally intensive tasks on the login node it would lead to the system being unusable.</p> <p>Compute nodes on the other hand are built to deal with computationally intensive tasks. They have 64 cpus and 512gb of RAM. However you cannot use them directly like you used the login node. We use a program called slurm as our cluster management and job scheduling system. Slurm provides a program called <code>srun</code> which can be used to run your scripts on the compute nodes. Let's run this now.</p> <pre><code>$ srun --ntasks 1 --cpus-per-task 1 ./hello_world.sh\nsrun: job 27870 queued and waiting for resources\nsrun: job 27870 has been allocated resources\nHello world!\n</code></pre> <p>Here we've used the srun command with two flags. The first flag says we only have one task to run and the second indicates that we only want one cpu for this task. Specifying these flags means that other users will be able to run programs on the unused cpus on the same node. There are many more flags for srun which might be useful for your work. Check out the docs for more information.</p> <p>It's great that we can now run programs on the compute nodes, but what if we want to leave a job running for a long time and don't want to stay logged in while it's running? This is where the <code>sbatch</code> command comes in. <code>sbatch</code> lets you submit a job to ENUCC and slurm will run it when the resources become available. Once you submit a job with <code>sbatch</code> you do not need to stay logged in. A batch file is just like a regular bash file with some special directives. Let's edit <code>hello_world.sh</code> to convert it to a batch script.</p> <pre><code>#!/bin/bash\n#SBATCH --ntasks 1\n#SBATCH --cpus-per-task 1\n\necho Hello world!\n</code></pre> <p>Now all we need to do is use the <code>sbatch</code> command to submit it to the job queue.</p> <pre><code>$ sbatch hello_world.sh\nSubmitted batch job 27872\n</code></pre> <p>It's been submitted, but where is our output? Since we might not be logged in when it runs, slurm will not allow the job to print to our terminal. Instead it will redirect any output to a file. That file will be in the working directory from which you call the <code>sbatch</code> command. We can see our output by using the cat command on the output file, which by default is called <code>slurm-&lt;JOB_ID&gt;.out</code>.</p> <pre><code>$ cat slurm-27872.out\nHello world!\n</code></pre>"},{"location":"tutorials/01-hello-world/#how-is-this-useful","title":"How is this useful?","text":"<p>The process you used to run this simple script on the cluster is exactly the same as the process you'd use to run any script which would run on a single node. You can change the bash script to run python scripts, organise files or otherwise automate your computational workloads.</p> <p>Looking for a next step? Try running some scripts you'd normally run locally on ENUCC instead. Use a batch script to run them overnight.</p>"},{"location":"tutorials/02-running-python-analysis/","title":"Running Python Analysis","text":""},{"location":"tutorials/02-running-python-analysis/#description","title":"Description","text":"<p>This project covers how to move long-running scripts from running on your local machine to running on ENUCC. We will learn how to run a series of unconnected jobs in parallel using <code>sbatch</code> and <code>srun</code> and then we'll use the array flag to supply a range of values which will be parameters supplied to the script we're running.</p>"},{"location":"tutorials/02-running-python-analysis/#prerequisites","title":"Prerequisites","text":"<p>Ideally you should have run scripts using <code>srun</code> and <code>sbatch</code> before (if not check out the hello world project). </p>"},{"location":"tutorials/02-running-python-analysis/#initial-script","title":"Initial script","text":"<p>Let's say we have some python analysis that runs on a laptop, but takes some time to run. We want to run it 20 times with different parameters. Ideally we would set up a script to submit all 20 runs to ENUCC. We then come back in a few day and check our results.</p> <p>As a simple example, let's say we want to calculate some statistics about the product of the outcomes of two dice rolls. We also want to see how these statistics change depending on how many sides the dice have. Here is a simple script that takes the number of sides of the dice and the number of trials as command line arguments and prints the mean, mode and standard deviation of the product of the two dice rolls.</p> <pre><code>import sys\nimport random\nimport statistics\n\nn_sides = int(sys.argv[1])\nn_trials = int(sys.argv[2])\n\nfirst_die_outcomes = []\nsecond_die_outcomes = []\n\nfor i in range(n_trials):\nfirst_die_outcomes.append(random.randint(1, n_sides))\nsecond_die_outcomes.append(random.randint(1, n_sides))\n\nproducts = [a * b for a, b in zip(first_die_outcomes, second_die_outcomes)]\n\noutput = (f\"------------------------------------------------\\n\"\n      f\"Product of two {n_sides} sided dice. {n_trials} trials\\n\"\n      f\"------------------------------------------------\\n\"\n      f\"Mean: {statistics.mean(products)}\\n\"\n      f\"Median: {statistics.median(products)}\\n\"\n      f\"Mode: {statistics.mode(products)}\\n\")\n\nprint(output)\n</code></pre>"},{"location":"tutorials/02-running-python-analysis/#getting-files-onto-enucc","title":"Getting files onto ENUCC","text":"<p>The first thing we need to do is get the files from our local computer to enucc. Ideally you should be using git to manage your projects, with a remote repository on a hosting site such as github or gitlab. If you already have this setup it's a simple matter to clone the repository. I have put all of the code we need into a git repository so you should try cloning this now.</p> <pre><code>$ git clone git@github.com:SCEBE-Technicians/python-analysis-tutorial.git\n</code></pre> <p>If you aren't using git you can transfer files from either Linux or MacOS using either scp or rsync. The syntax is as follows:</p> <pre><code>$ scp /path/to/local/file 400XXXXX@login.enucc.napier.ac.uk:/path/to/server/file\n</code></pre> <p>or</p> <pre><code>$ rsync /path/to/local/file 400XXXXX@login.enucc.napier.ac.uk:/path/to/server/file\n</code></pre> <p>You can also use these programs to copy files from the server to your local machine. If you are on windows you can use scp from powershell.</p> <pre><code>&gt; scp \\path\\to\\local\\file 400XXXXX@login.enucc.napier.ac.uk:/path/to/server/file\n</code></pre>"},{"location":"tutorials/02-running-python-analysis/#where-should-i-put-data-files","title":"Where should I put data files?","text":"<p>This will be covered more thoroughly in a future project but the short answer is that you want to put large data files in <code>sharedscratch</code> for long term-storage or in <code>localscratch</code> for short-term access. <code>localscratch</code> is faster but will get wiped after 30 days.</p>"},{"location":"tutorials/02-running-python-analysis/#using-anaconda","title":"Using Anaconda","text":"<p>On ENUCC, python is managed with anaconda and so we have to set up our system to use anaconda. You simply need to load the correct module and initialise conda. You'll have to restart your shell (by logging out and in) after running these two commands.</p> <pre><code>$ module load apps/anaconda3\n$ conda init bash\n</code></pre> <p>To run the script for 1000 trials of 6 sided dice we simply run</p> <pre><code>$ python product_of_two_dice_analysis.py 6 1000\n------------------------------------------------\nProduct of two 10 sided dice. 10 trials\n------------------------------------------------\nMean: 29.4\nMedian: 23.5\nMode: 30\n</code></pre>"},{"location":"tutorials/02-running-python-analysis/#running-the-script","title":"Running the script","text":"<p>To run the same script on ENUCC we can use <code>srun</code>. Here we run it for a reasonably large number of trials.</p> <pre><code>$ srun python product_of_two_dice_analysis.py 10 10000000\nsrun: job 27947 queued and waiting for resources\nsrun: job 27947 has been allocated resources\n------------------------------------------------\nProduct of two 10 sided dice. 10000000 trials\n------------------------------------------------\nMean: 30.2445156\nMedian: 24.0\nMode: 6\n</code></pre> <p>So now we want to run it many times, for different number of sides and different numbers of trials. For this we'll have to write a batch script.</p> <pre><code>#!/bin/bash\n\nstart_time=`date +%s.%N`\n\npython product_of_two_dice_analysis.py 6 10000000\n\n\nend_time=`date +%s.%N`\nruntime=$(echo \"$end_time - $start_time\" | bc)\nprintf \"This job ran in %s seconds\\n\\n\" $runtime\n</code></pre> <p>Here I've used date and bc to calculate the runtime of the of the program. This will be important later when we're trying to guage how well our script will scale.</p> <p>As usual, we submit it with sbatch and see our output in the output file.</p> <pre><code>$ sbatch job_script.sh\nSubmitted batch job 27949\n\n$ cat slurm-27949.out\n------------------------------------------------\nProduct of two 6 sided dice. 1000 trials\n------------------------------------------------\nMean: 12.493\nMedian: 10.0\nMode: 6\n\nThis job ran in 11.822068605 seconds\n</code></pre> <p>Great. We're now in a position that we can modify our script to run our job a few times. Let's start by running 10 identical copies of it. Modify the bash script like so:</p> <pre><code>#!/bin/bash\n\nstart_time=`date +%s.%N`\n\npython product_of_two_dice_analysis.py 6 10000000\npython product_of_two_dice_analysis.py 6 10000000\npython product_of_two_dice_analysis.py 6 10000000\npython product_of_two_dice_analysis.py 6 10000000\npython product_of_two_dice_analysis.py 6 10000000\npython product_of_two_dice_analysis.py 6 10000000\npython product_of_two_dice_analysis.py 6 10000000\npython product_of_two_dice_analysis.py 6 10000000\npython product_of_two_dice_analysis.py 6 10000000\npython product_of_two_dice_analysis.py 6 10000000\n\nend_time=`date +%s.%N`\nruntime=$(echo \"$end_time - $start_time\" | bc)\nprintf \"This job ran in %s seconds\\n\\n\" $runtime\n</code></pre> <p>The output of the job script is quite long this time, so let's just see how long it ran for this time.</p> <pre><code>$ cat slurm-27951.out\n...\nThis job ran in 120.238923031 seconds\n</code></pre> <p>This has taken around 10 times longer than running the analysis once which is more or less what might be expected. ENUCC has a great deal of resources, but the script as we've written it requests only one CPU on one node and then runs the python script steps one after the other. There is no reason why we shouldn't run the python steps all at once, but we don't want to submit 10 different job. Instead we can run each python step as a separate task and assign one cpu to each task. We also have to specify how much memory each task should use since the default would request all available memory.</p>"},{"location":"tutorials/02-running-python-analysis/#parallelising-the-job","title":"Parallelising the job","text":"<pre><code>#!/bin/bash\n#SBATCH --ntasks 10\n#SBATCH --nodes 1\n#SBATCH --cpus-per-task 1\n\nstart_time=`date +%s.%N`\n\nsrun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\nsrun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\nsrun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\nsrun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\nsrun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\nsrun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\nsrun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\nsrun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\nsrun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\nsrun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\n\nwait\n\nend_time=`date +%s.%N`\nruntime=$(echo \"$end_time - $start_time\" | bc)\nprintf \"This job ran in %s seconds\\n\\n\" $runtime\n</code></pre> <p>To shorten the script and make it more readable we can also put our execution tasks in a loop.</p> <pre><code>#!/bin/bash\n#SBATCH --ntasks 10\n#SBATCH --nodes 1\n#SBATCH --cpus-per-task 1\n\nstart_time=`date +%s.%N`\n\nfor x in {1..10}\ndo\n  srun --exclusive --ntasks 1 python product_of_two_dice_analysis.py 6 10000000 &amp;\ndone\n\nwait\n\nend_time=`date +%s.%N`\nruntime=$(echo \"$end_time - $start_time\" | bc)\nprintf \"This job ran in %s seconds\\n\\n\" $runtime\n</code></pre> <p>Things are getting quite complicated now so let's take stock of what's happening. In the first few lines we specify that we'll be running 10 tasks on one node with one cpu per task. Instead of simply executing our code with <code>python script.py</code> now we use <code>srun</code> which is part of the slurm ecosystem. We use <code>srun</code> because we can specify what resources we want to use by passing in different flags. The <code>--exclusive</code> flag says that this job will have exclusive access to the requested resources. <code>--ntasks</code> says that this is only a single task. Then we supply the commands to run. Finally we add the <code>&amp;</code> character which runs the <code>srun</code> command in the background. This is necessary so that it can execute the next command before the current one has finished.</p> <p>When we run this script we can see the runtime is massively improved.</p> <pre><code>$ sbatch job_script.sh\n...\nThis job ran in 12.563785716 seconds \n</code></pre> <p>Note that as long as we have free resources on which to run the scripts, we can easily expand beyond running 10 copies and it will still take around 12 seconds. If we go beyond 64 tasks we will require more than one node.</p>"},{"location":"tutorials/02-running-python-analysis/#using-job-array","title":"Using job array","text":"<p>Now let's say we want to run this same script but change the number of sides each dice take, running it for 2, 4, 6 sides. Yes we could just use bash loops, but there is a simpler way to do this using the <code>--array</code> flag. Let's see what it looks like:</p> <pre><code>#!/bin/bash\n#SBATCH --ntasks 1\n#SBATCH --array=2,4,6\n#SBATCH --cpus-per-task 1\n#SBATCH --output=variable-sided-dice-result.out\n\npython product_of_two_dice_analysis.py $SLURM_ARRAY_TASK_ID 10000000 &amp;\n</code></pre> <p>This is much simpler than the previous script. Using the <code>--array</code> flag we specify what values we want to run the job for and we use <code>$SLURM_ARRAY_TASK_ID</code> to access those values in the script. When we submit this job it creates 3 versions of the job. We can see that the job id has an array appended with the specified values.</p> <pre><code>$ squeue\n         JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n 27995_[2,4,6]     nodes job_scri 40019142 PD       0:00      1 (Priority)\n</code></pre> <p>The <code>--array</code> flag is not limited to specifying individual values. There are a few different options for specifying values depending on your needs.</p> <code>--array</code> description 1,2,3 1,2,3 1-100 1,2 ... 100 1-100:2 1, 3, 5 ... 99 1-100%5 1,2 ... 100, running 5 jobs simultaneously"},{"location":"tutorials/02-running-python-analysis/#conclusion","title":"Conclusion","text":"<p>We've seen a few different ways that we can launch jobs. While launching multiple jobs with the array flag will likely be the most common usage, it is good to know how to launch individual jobs within a batch file so that we can have finer control over what scripts are running.</p>"},{"location":"tutorials/03-simple-neural-network/","title":"Simple Neural Network","text":""},{"location":"tutorials/03-simple-neural-network/#description","title":"Description","text":"<p>This is a barebones machine learning project. It involves getting some data, storing it in an appropriate location, installing packages with conda and running the training on ENUCC.</p> <p>This tutorial is largely based on a tutorial on machinelearningmastery.com. </p>"},{"location":"tutorials/03-simple-neural-network/#prerequisites","title":"Prerequisites","text":"<p>You should be familiar with writing and submitting jobs to ENUCC and also have some knowledge of python. This is not a machine learning tutorial, but meant to show you how to run a machine learning project on ENUCC.</p> <p>We start by getting the data by downloading it using the wget program.</p> <pre><code>$ mkdir ~/sharedscratch/diabetes-dataset\n$ cd ~/sharedscratch/diabetes-dataset\n$ wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\n</code></pre> <p>Note that I choose to store the data in the <code>sharedscratch</code> folder. This is a shared directory which does not have a storage quota. If the speed of loading data is a big factor for your work then you should use <code>localscratch</code> instead, but note that it is wiped at regular intervals and should not be used for long-term storage.</p> <p>We then create a folder to container our analysis</p> <pre><code>$ mkdir -p ~/projects/diabetes-analysis\n$ cd ~/projects/diabetes-analysis\n</code></pre> <p>In order to run the analysis we have to have python, numpy and tensorflow installed. We do this in a conda environment called <code>ml-example</code>. If you haven't used conda on ENUCC before then you'll have to set it up by running the following.</p> <pre><code>$ module load apps/anaconda3\n$ conda init bash\n</code></pre> <p>Once conda is configured, you can create the environment.</p> <pre><code>$ conda create -n ml-example\n$ conda activate ml-example\n$ conda install python=3.11 numpy tensorflow\n</code></pre> <p>To train the machine learning model we'll use the following script</p> <pre><code>from numpy import loadtxt\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom pathlib import Path\n\ndata_path = Path.home() / \"sharedscratch\" / \"diabetes-dataset\" / \"pima-indians-diabetes.data.csv\"\ndataset = loadtxt(data_path, delimiter=',')\nx = dataset[:, 0:8]\ny = dataset[:, 8]\n\nmodel = Sequential()\nmodel.add(Dense(12, input_shape=(8,), activation='relu'))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.fit(x, y, epochs=50, batch_size=10)\n\n_, accuracy = model.evaluate(x, y)\nprint('Accuracy: %.2f' % (accuracy*100))\n</code></pre> <p>The batch file to submit the job is quite straightforward</p> <pre><code>#!/bin/bash\n\npython model.py\n</code></pre> <p>And that's it! Sin \u00e9.</p>"},{"location":"tutorials/04-jupyter-notebook/","title":"Jupyter Notebook","text":""},{"location":"tutorials/04-jupyter-notebook/#description","title":"Description","text":"<p>Many people use jupyter notebooks for writing and running python code - especially when doing data science tasks. This tutorial will show you how to take a jupyter notebook which is running on your own computer and run it on ENUCC.</p>"},{"location":"tutorials/04-jupyter-notebook/#prerequisites","title":"Prerequisites","text":"<p>You should know the basics of creating and submitting jobs to ENUCC. You should also know a bit about creating and running jupyter notebooks.</p> <p>I have set up an example notebook which can be obtained from github. In a real usage scenario we might expect the notebook to do some heavy number crunchy, or neural network training, but here it simply generates a graph and puts it in the figures subdirectory. We're happy that the notebook works on the local machine, but it takes a long time and so we'd rather offload the effort to ENUCC. The first step is to clone the repository:</p> <pre><code>$ cd projects\n$ git clone git@github.com:SCEBE-Technicians/ENUCC-jupyter-notebook-example.git\n</code></pre> <p>If you've never used conda on ENUCC before then you'll need to do some setup. First load the conda module and then initialise it.</p> <pre><code>$ module load apps/anaconda3\n$ conda init bash\n</code></pre> <p>You'll have to restart the shell for these changes to take effect (the easiest way is to log out and back in).</p> <p>Now, in the repository that you cloned previously, there is a conda environment already defined. You can create a new conda environment from an <code>env.yml</code> file by running</p> <pre><code>$ cd ~/projects/ENUCC-jupyter-notebook-example\n$ conda env create -f env.yml\n$ conda activate jupyter-example\n</code></pre> <p>This environment has all the dependencies you'll need already installed, namely jupyter, numpy and matplotlib.</p> <p>We are now in a position to execute the notebook. This is really quite straightforward. If there is a free compute node (run <code>squeue</code> to check) then we can run it as follows.</p> <pre><code>$ srun jupyter execute NoisySine.ipynb\nsrun jupyter execute NoisySine.ipynb\n[NbClientApp] Executing NoisySine.ipynb\n...\n</code></pre> <p>You can then check in the <code>./figures</code> directory to see that a new figure has been added.</p> <p>As usual, <code>srun</code> is handy if there is a free compute node and your script won't take too long to run, but it's better to create a batch script if it is a long-running script, or if you expect it to be queued for a while.</p> <pre><code>#!/bin/bash\n#SBATCH --ntasks 1\n#SBATCH --nodes 1\n#SBATCH --cpus-per-task 1\n\njupyter execute NoisySine.ipynb\n</code></pre>"},{"location":"tutorials/04-jupyter-notebook/#configuring-conda","title":"Configuring conda","text":"<p>You have some options for using conda with ENUCC. If you find your environments are getting to be too big then you can move your modules to a location within shared scratch</p> <pre><code>$ conda config --add pkg_dirs sharedscratch/.conda/pkgs\n$ conda config --add envs_dirs sharedscratch/.conda/envs\n</code></pre> <p>You can also disable the base env being activated upon login.</p> <pre><code>$ conda config --set auto_activate_base false\n</code></pre>"},{"location":"tutorials/message-passing-programming/","title":"Message Passing Programming","text":"<p> This is a work in progress. </p>"},{"location":"tutorials/message-passing-programming/#description","title":"Description","text":"<p>Compute nodes on ENUCC each have up to 64 cores which have access to shared memory. When looking to run a job with more than 64 cores, we have to use multiple nodes, but separate nodes do not have shared memory. We need to use message passing programming to send information between nodes. We will be exploring the basics of message passing programming with MPI. The goal with this tutorial is to give you a taste of MPI programming along with an understanding of its capabilities so that you can understand whether it will be useful for future applications. This is not a comprehensive course.</p>"},{"location":"tutorials/message-passing-programming/#prerequisites","title":"Prerequisites","text":"<p>You should be comfortable using slurm to submit batch scripts. You should have a basic knowledge of the C programming language. Here is a quick reference in case you aren't used to programming in C. If have strong programming skills in a higher level programming language such as python you will probably still be able to follow along but some concepts in C such as pointers do not have an analogue in python.</p> <p>Message passing programming allows us to run a program with a very large number of cpus which do not necessarily have access to shared memory. Our program runs on several processes which can communicate with each other by passing messages. Let's start with a basic hello world program. Create a <code>hello_world.c</code> file with the following contents</p> <pre><code>#include &lt;stdio.h&gt;\n#include &lt;mpi.h&gt;\n\nint main (int argc, char **argv) {\n    MPI_Init(NULL, NULL);\n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank);\n\n    printf(\"Hello world from rank %d\\n\", world_rank);\n\n    return MPI_Finalize();\n}\n</code></pre> <p>in order to compile mpi programs we use the <code>mpicc</code> compile rather than <code>gcc</code>. You'll also have to load the mpi module.</p> <pre><code>$ module load mpi\n$ mpicc hello_world.c -o hello_world\n</code></pre> <p>We use the <code>mpirun</code> command to run it. As per usual, we need to use either <code>sbatch</code> or <code>srun</code> to run programs on the compute nodes. Here's a batch file which will run the compiled <code>hello_world</code> binary.</p> <pre><code>#!/bin/bash -l\n#SBATCH --export=ALL\n#SBATCH -o hello_world.output\n#SBATCH -J mpi-test\n#SBATCH --time=0-1\n#SBATCH --mem=1024\n#SBATCH --nodes=1\n#SBATCH --ntasks=4\n\n# module load mpi/openmpi\n\nmpirun hello_world\n</code></pre> <p>After submitting the script with <code>sbatch</code> we can inspect the output.</p> <pre><code>$ cat hello_world.output\nHello world from rank 0\nHello world from rank 1\nHello world from rank 2\nHello world from rank 3\n</code></pre> <p>We have run the task with four processes (by specifying <code>ntasks</code>) and it has printed a line on each of those processes.</p> <p>So now we have four processes running simultaneously, how can we get them to talk to each other? We send and receive messages!</p>"},{"location":"tutorials/message-passing-programming/#passing-a-message","title":"Passing a message","text":"<p>Let's write a very basic program which will send an integer from rank 0 to rank 1. Let's look at the program first and then we'll look at what's going on.</p> <pre><code>#include &lt;mpi.h&gt;\n#include &lt;stdio.h&gt;\n\nint main(int argc, char** argv) {\n    MPI_Init(NULL, NULL);\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);\n\n    if (rank == 0) {\n        int send = 5;\n        printf(\"Sending the number %d.\\n\", send);\n        MPI_Send(&amp;send, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);\n    } else if (rank == 1) {\n        int rec;\n        MPI_Recv(&amp;rec, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        printf(\"Received the number %d.\\n\", rec);\n    }\n    MPI_Finalize();\n}\n</code></pre> <p>Here we've used a few new methods, <code>MPI_Send</code> and <code>MPI_Recv</code>. All MPI methods typically follow a similar pattern. You can see the method definitions in the appendix. </p> <p>We call <code>MPI_Send</code> when we want to send data from one process to another. Then from the receiving process we need to call <code>MPI_Recv</code>. Since the same code will run for every process, we use <code>if</code> statements to send the message only on rank 0 and receive only on rank 1. There are some examples at the end of this tutorial which illustrate more interesting ways to use blocking communication.</p>"},{"location":"tutorials/message-passing-programming/#using-probe-to-get-the-message-size","title":"Using probe to get the message size","text":"<p>Let's say we want to send a string, but we don't know what size it's going to be. We can use <code>MPI_Probe</code> to get the size of the message.</p> <pre><code>#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n#include &lt;mpi.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;time.h&gt;\n\nint main(int argc, char** argv) {\n\n    MPI_Init(NULL, NULL);\n\n    // Initialise rank and world size\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);\n\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &amp;world_size);\n\n    // Need at least 2 processes\n    if (world_size &lt; 2) {\n        MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n\n        if (rank == 0) {\n        // Choose one of four animals\n        srand(time(NULL));\n        char animals[4][10] = {\"dog\", \"cat\", \"rat\", \"gorilla\"};\n        char animal[10];\n        int index = rand() % 4;\n        strcpy(animal, animals[index]);\n\n        printf(\"\\033[31mSending %s, of size %d\\n\", animal, strlen(animal));\n\n        MPI_Send(animal, strlen(animal), MPI_CHAR, 1, 0, MPI_COMM_WORLD);\n    } else if (rank == 1) {\n        // Get message status\n        MPI_Status stat;\n        MPI_Probe(0, 0, MPI_COMM_WORLD, &amp;stat);\n\n        // Get message length from status\n        int message_length;\n        MPI_Get_count(&amp;stat, MPI_CHAR, &amp;message_length);\n        printf(\"\\033[32mReceiving message of length %d\\n\", message_length);\n\n        // Receive message\n        char message[message_length];\n        MPI_Recv(&amp;message, message_length, MPI_CHAR, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        printf(\"Received %s from sender\\033[0m\\n\", message);\n    }\n\n    MPI_Finalize();\n}\n\n</code></pre>"},{"location":"tutorials/message-passing-programming/#broadcast","title":"Broadcast","text":"<p>The broadcast is one of the collective communication methods. The method is called on all processes. You provide the method a data buffer and specify the root process (by rank id). Broadcast will send a message to all other processes so that the variable is synchronised between processes. Here is an example where a random number is generated on process 0 and this is sent to all other processes.</p> <pre><code>#include &lt;mpi.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;time.h&gt;\n\nint main(int arg, char** argv) {\n    srand(time(NULL));\n\n    MPI_Init(NULL, NULL);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);\n    int i;\n    if (rank == 0) {\n            i = rand();\n        printf(\"sending %d to all processes\\n\", i);\n    }\n\n    MPI_Bcast(&amp;i, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    printf(\"%d\\n\", i);\n    MPI_Finalize();\n}\n</code></pre>"},{"location":"tutorials/message-passing-programming/#scatter-and-gather","title":"Scatter and Gather","text":"<p>Lets say we have an algorithm where we need to iterate over an array and perform an independent computation on each item. Here is an example program where we calculate the sum of an array of 100 integers. I've added a sleep after each step to simulate a heavier computation.</p> <pre><code>#include &lt;stdlib.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;sys/time.h&gt;\n#include &lt;unistd.h&gt;\n\n#define ARRAY_SIZE 100\n\ndouble time_diff(struct timeval* start, struct timeval* end) {\n    double elapsed_time = (double)(end-&gt;tv_sec - start-&gt;tv_sec);\n    elapsed_time += (double)(end-&gt;tv_usec - start-&gt;tv_usec) / 1000000;\n    return elapsed_time;\n}\n\nvoid fill_array(int *arr) {\n    for (int i = 0; i&lt;ARRAY_SIZE; i++) {\n        arr[i] = rand();\n    }\n}\n\nint slow_sum_array(int *arr) {\n    int sum = 0;\n    for (int i = 0; i&lt;ARRAY_SIZE; i++) {\n        sum += arr[i];\n        usleep(10000);\n    }\n    return sum;\n}\n\nint main(int argc, char* argv[]) {\n    // Seed the rng so every run gives the same result\n    srand(0);\n\n    // Create an array of random numbers\n    int list_of_random_numbers[ARRAY_SIZE];\n    fill_array(list_of_random_numbers);\n\n    // Measure the time before doing the calculation\n    struct timeval start;\n    struct timeval end;\n    gettimeofday(&amp;start, 0);\n\n    int sum = slow_sum_array(list_of_random_numbers);\n\n    // Measure the time after doing the calculation\n    gettimeofday(&amp;end, 0);\n    double elapsed_time = time_diff(&amp;start, &amp;end);\n\n    printf(\"Sum: %d\\n\", sum);\n    printf(\"Elapsed time: %f\\n\", elapsed_time);\n\n    return 0;\n}\n</code></pre> <p>The batch script to run this can be found on the corresponding repository.</p> <p>In order to parallelise this task, we need to split the <code>list_of_random_numbers</code> array and distribute it between the processes. We use the <code>MPI_Scatter</code> function to achieve this. We then compute the sum of the subarrays and use <code>MPI_Gather</code> to gather the results to an array on rank 0. We can sum this subarray to calculate the results. Here is the full program.</p> <pre><code>#include &lt;stdlib.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;sys/time.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;mpi.h&gt;\n\n#define ARRAY_SIZE 100\n\nvoid mpi_setup(int *world_size, int *rank) {\n    MPI_Init(NULL, NULL);\n    MPI_Comm_rank(MPI_COMM_WORLD, rank);\n    MPI_Comm_size(MPI_COMM_WORLD, world_size);\n}\n\ndouble time_diff(struct timeval* start, struct timeval* end) {\n    double elapsed_time = (double)(end-&gt;tv_sec - start-&gt;tv_sec);\n    elapsed_time += (double)(end-&gt;tv_usec - start-&gt;tv_usec) / 1000000;\n    return elapsed_time;\n}\n\nvoid fill_array(int *arr) {\n    for (int i = 0; i&lt;ARRAY_SIZE; i++) {\n        arr[i] = rand();\n    }\n}\n\nint slow_sum_array(int *arr, int size) {\n    int sum = 0;\n    for (int i = 0; i&lt;size; i++) {\n        sum += arr[i];\n        usleep(10000);\n    }\n    return sum;\n}\n\nint main(int argc, char* argv[]) {\n    int rank, world_size;\n    mpi_setup(&amp;world_size, &amp;rank);\n\n    // Create an array of random numbers\n    int list_of_random_numbers[ARRAY_SIZE];\n    fill_array(list_of_random_numbers);\n\n    long sum = 0;\n\n    // Measure the time before doing the calculation\n    struct timeval start;\n    struct timeval end;\n    if (rank == 0) {\n        gettimeofday(&amp;start, 0);\n    }\n\n    // Distribute elements to all processes\n    int elements_per_proc = ARRAY_SIZE/world_size;\n    int *sub_array = malloc(sizeof(int) * elements_per_proc);\n    MPI_Scatter(list_of_random_numbers, elements_per_proc, MPI_INT, sub_array, elements_per_proc, MPI_INT, 0, MPI_COMM_WORLD);\n\n    // Sum each subarray\n    sum = slow_sum_array(sub_array, elements_per_proc);\n\n    // Gather all sums into array\n    int *sub_sums;\n    if (rank == 0) {\n        sub_sums = (int *)malloc(sizeof(float) * world_size);\n    }\n    MPI_Gather(&amp;sum, 1, MPI_INT, sub_sums, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    int aggregate_sum = 0;\n    if (rank == 0) {\n        for (int i = 0; i&lt;world_size; i++) {\n            aggregate_sum += sub_sums[i];\n        }\n    }\n\n    // Measure the time after doing the calculation\n\n    if (rank == 0) {\n        gettimeofday(&amp;end, 0);\n        double elapsed_time = time_diff(&amp;start, &amp;end);\n        free(sub_sums);\n        printf(\"Sum: %d\\n\", aggregate_sum);\n        printf(\"Elapsed time: %f\\n\", elapsed_time);\n    }\n\n    MPI_Finalize();\n}\n</code></pre> <p>The same result can be achieved using <code>MPI_Reduce</code> instead.</p>"},{"location":"tutorials/message-passing-programming/#asynchronous-message-passing","title":"Asynchronous message passing","text":""},{"location":"tutorials/message-passing-programming/#examples","title":"Examples","text":""},{"location":"tutorials/message-passing-programming/#ping-pong","title":"Ping Pong","text":"<pre><code>#include &lt;mpi.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nint main(int argc, char** argv) {\n    const int PING_PONG_LIMIT = 10;\n\n    MPI_Init(NULL, NULL);\n\n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank);\n\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &amp;world_size);\n\n    if (world_size != 2) {\n        MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n\n    int ping_pong_count = 0;\n    int partner_rank = (world_rank + 1) % 2;\n\n    while(ping_pong_count &lt; PING_PONG_LIMIT) {\n        if (world_rank == ping_pong_count % 2) {\n            ping_pong_count++;\n\n            MPI_Send(&amp;ping_pong_count, 1, MPI_INT, partner_rank, 0, MPI_COMM_WORLD);\n            printf(\"%d sent from %d to %d\\n\", ping_pong_count, world_rank, partner_rank);\n        } else {\n            MPI_Recv(&amp;ping_pong_count, 1, MPI_INT, partner_rank, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            printf(\"%d received from %d on %d\\n\", ping_pong_count, world_rank, partner_rank);\n        }\n    }\n    MPI_Finalize();\n}\n</code></pre> <p>Challenge: implement a ring-pong program where instead of two processes, you have an arbitrary number of processes which pass a value around in a ring.</p>"},{"location":"tutorials/message-passing-programming/#next-steps","title":"Next steps","text":""},{"location":"tutorials/message-passing-programming/#useful-resources","title":"Useful resources","text":"<ul> <li>MPI Tutorial - if you are looking to get into message passing programming then doing this course in its entirety is almost certainly the best place to start.</li> <li>Microsoft MPI Reference - a nice reference guide.</li> </ul>"},{"location":"tutorials/message-passing-programming/#appendix","title":"Appendix","text":""},{"location":"tutorials/message-passing-programming/#method-definitions","title":"Method definitions","text":"<pre><code>MPI_Send(\n    void *buf, \n    int count, \n    MPI_Datatype datatype,\n    int dest, \n    int tag, \n    MPI_Comm comm\n);\n</code></pre> <ul> <li><code>buf</code> - a pointer to the data to be sent.</li> <li><code>count</code> - the number of elements in the buffer. If sending an array, it would be the length of the array.</li> <li><code>datatype</code> - the type of the data being sent. Must be one of the predefined datatypes.</li> <li><code>dest</code> - the rank of the receiver.</li> <li><code>tag</code> - a tag for distinguishing message types - set to 0 for most normal usage.</li> <li><code>comm</code> - the mpi communicator. Set to <code>MPI_COMM_WORLD</code> for most normal usage.</li> </ul> <pre><code>MPI_Recv(\n    void *buf,\n    int count,\n    MPI_Datatype datatype,\n    int source,\n    int tag,\n    MPI_Comm comm,\n    MPI_Status *status\n);\n</code></pre> <ul> <li><code>buf</code> - a pointer to the location where the received data will be stored.</li> <li><code>count</code> - the number of elements in the buffer. If sending an array, it would be the length of the array.</li> <li><code>datatype</code> - the type of the data being sent. Must be one of the predefined datatypes.</li> <li><code>source</code> - the rank of the sender.</li> <li><code>tag</code> - a tag for distinguishing message types - set to 0 for most normal usage.</li> <li><code>comm</code> - the mpi communicator. Set to <code>MPI_COMM_WORLD</code> for most normal usage.</li> <li><code>status</code> - contains a pointer to an MPI_Status.</li> </ul>"},{"location":"tutorials/matlab/05-using-matlab/","title":"Using Matlab","text":""},{"location":"tutorials/matlab/05-using-matlab/#description","title":"Description","text":"<p>This is a short project which shows how to run a small script using matlab on ENUCC.</p>"},{"location":"tutorials/matlab/05-using-matlab/#prerequisites","title":"Prerequisites","text":"<p>It is highly recommended that you do the Hello World tutorial before doing this tutorial. </p> <p>In code blocks, lines starting with <code>$</code> are lines that you execute. Don't type the dollar since it represents the prompt. Lines that don't start with a <code>$</code> are lines that you should see after executing the command.</p> <p>In order to use matlab the first thing you'll have to do is load the matlab module.</p> <pre><code>$ module load apps/matlab\napps/matlab/r2023b\n |\n  OK\n</code></pre> <p>Then let's create a folder to store our work and cd into it</p> <pre><code>$ mkdir -p projects/matlab-example\n$ cd projects/matlab-example\n</code></pre> <p>Create a file called <code>script.m</code> with the following contents.</p> <pre><code>x = linspace(0, 10);\n\ny = sin(x);\n\nplot(x, y);\n\nsaveas(gcf, 'graph', 'svg');\nfprintf('Saved figure as graph.svg\\n')\nexit\n</code></pre> <p>The batch script to submit the job is fairly simple. Put the following into a file called <code>batch.sh</code>.</p> <pre><code>#!/bin/bash\n\nmatlab -nodesktop -r script\n</code></pre> <p>You can submit the job to the queue by running</p> <pre><code>$ sbatch batch.sh\n</code></pre> <p>We need to pass the <code>-nodesktop</code> flag to stop it from starting the GUI and the <code>-r</code> flag to specify that we want to run a script.</p> <p>You'll see the ouput in an file called <code>job-XXXXX.out</code></p>"},{"location":"tutorials/matlab/saving-and-loading-with-matlab/","title":"Saving and Loading your Workspace","text":""},{"location":"tutorials/matlab/saving-and-loading-with-matlab/#description","title":"Description","text":"<p>This tutorial shows a simple workflow for doing intensive computations on the server and accessing the results from your local machine.</p>"},{"location":"tutorials/matlab/saving-and-loading-with-matlab/#prerequisites","title":"Prerequisites","text":"<p>You should have some knowledge of matlab and how to write and submit batch files using slurm.</p> <p>When working with matlab on ENUCC, you will probably want to do all of the intensive computation on the server. However, when exploring the results and producing figures, it is good to be able to work on your own machine. This tutorial will illustrate how to save results from a simulation to a file and then how to load that file from another location in order to access the results.</p> <p>The example project we'll use will be a program which computes every prime number up to a given integer. The project is split into two separate files. First create a directory to store the project:</p> <pre><code>$ mkdir ~/projects/prime-finder\n$ cd ~/projects/prime-finder\n$ mkdir output\n</code></pre> <p>We also created a folder to store the output of the program.</p> <p>In order to use matlab we must load the correct module</p> <pre><code>$ module load apps/matlab\n</code></pre> <p>The code for this project is split into two separate files. The first file is called <code>is_integer_prime.m</code> and it contains code to check whether a given integer is prime or not.</p> <pre><code>function result = is_integer_prime(x)\n    for i = 2:ceil(sqrt(x))\n        if mod(x, i) == 0\n            result = false;\n            return\n        end\n    end\n    result = true;\nend\n</code></pre> <p>This is quite a simple function which will check if any number below the square root of x divides x exactly. If it does then it returns false. Otherwise it returns true. Note that when putting matlab functions into separate files it is important that the filename and function name are the same.</p> <p>The other file which we need will use the <code>is_integer_prime</code> function to check all numbers up to a given number. This file will be called <code>prime_finder.m</code> and have the following code</p> <pre><code>function [] = prime_finder(x)\n    list_of_primes = [];\n    for i = 1:x\n        if is_integer_prime(i)\n            list_of_primes = [list_of_primes i];\n            fprintf('%d is prime\\n', i)\n        else\n            fprintf('%d is not prime\\n', i)\n        end\n    end\n    save(sprintf('output/primes_up_to_%d.MAT', x))\n    exit;\nend\n</code></pre> <p>This is a bit more complicated. We have an empty array called <code>list_of_primes</code>. We loop through all numbers up to x and if the number is prime we append the value to <code>list_of_primes</code>. We also print some information to the console so we can see how the program is running. If efficiency were a big concern then it would be a good idea to remove the print statements.</p> <p>Finally we call the function <code>save</code> which will save all the variables in the workspace to a file called <code>output/primes_up_to_x.MAT</code> where it replaces x with the value. </p> <p>In order to run this code we create a batch file called <code>batch.sh</code></p> <pre><code>#!/bin/bash\n\nmatlab -nodesktop -r \"prime_finder(1000000)\"\n</code></pre> <p>Queue the batch file to be executed using <code>sbatch batch.sh</code>.</p> <p>You can see the output has been saved by running <code>ls output</code>. You can also see the printed statements in a file called <code>slurm-xxxx.out</code>.</p> <p>We can move the output file from the server to the local machine either using the MobaXTerm file browser or something like rsync or scp. Move the file to a Matlab workspace and you can use the following script to plot a histogram showing the density of primes. </p> <pre><code>function [] = plot()\n    load('output/primes_up_to_1000000.MAT');\n    histogram(list_of_primes);\n    saveas(gcf, 'figures/histogram_of_primes.svg');\nend\n</code></pre>"},{"location":"tutorials/message-passing-programming/01-hello-mpi/","title":"Hello MPI!","text":""},{"location":"tutorials/message-passing-programming/01-hello-mpi/#description","title":"Description","text":"<p>This is the first tutorial in a series which covers message passing programming. This tutorial looks at getting started with compiling and running an MPI program. All code can be found in this repository.</p>"},{"location":"tutorials/message-passing-programming/01-hello-mpi/#prerequisites","title":"Prerequisites","text":"<p>You should have a good understanding of C. If not then I recommend \"The C Programming Language\" by Ritchie and Kernighan.</p> <p>Message passing programming allows us to run a program with a very large number of cpus which do not necessarily have access to shared memory. Our program runs on several processes which can communicate with each other by sending messages which contain data. We will be using the OpenMPI implementation of the Message Passing Interface (MPI) standard to implement algorithms and run them on the cluster. Let's start with a basic hello world program. Create a <code>hello_world.c</code> file with the following contents</p> <pre><code>#include &lt;stdio.h&gt;\n#include &lt;mpi.h&gt;\n\nint main (int argc, char **argv) {\n    MPI_Init(NULL, NULL);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &amp;world_size);\n\n    printf(\"Hello world from rank %d of %d\\n\", rank, world_size);\n\n    return MPI_Finalize();\n}\n</code></pre> <p>Hopefully this program is fairly self explanatory but let's see what is included. Most of what is included here is boilerplate that needs to be included in any MPI program. We first need to include the mpi header file. Then we initialise MPI with the <code>MPI_Init(NULL, NULL)</code> command. MPI works by running several copies of the same process concurrently. We can differentiate each process by its rank - a number which acts as a unique identifier for each process. We find the rank by calling <code>MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank)</code>. The first argument specifies that we are addressing all processes. The second argument is a pointer to an int where the rank will be stored. It is a very common pattern with MPI functions that we first initialize a variable and then pass a pointer to that variable to a function which will initialise it.</p> <p>The exact same pattern is used to get <code>world_size</code> which tells how many processes the program is running on. Finally we have to run <code>MPI_Finalize()</code> at the end of the program to ensure that the process exits correctly.</p> <p>in order to compile mpi programs we use the <code>mpicc</code> compiler rather than <code>gcc</code>. You'll also have to load the mpi module.</p> <pre><code>$ module load mpi\n$ mpicc hello_world.c -o hello_world\n</code></pre> <p>We use the <code>mpirun</code> command to run it. As per usual, we need to use either <code>sbatch</code> or <code>srun</code> to run programs on the compute nodes. Here's a batch file which will run the compiled <code>hello_world</code> binary.</p> <pre><code>#!/bin/bash -l\n#SBATCH -o hello_world.output\n#SBATCH -J mpi-test\n#SBATCH --time=5\n#SBATCH --mem=256\n#SBATCH --nodes=1\n#SBATCH --ntasks=4\n\nmpirun hello_world\n</code></pre>"},{"location":"tutorials/message-passing-programming/01-hello-mpi/#exercises","title":"Exercises","text":"<p>Here are some simple ideas for getting practice with MPI:</p> <ul> <li>Write a program which prints a different message on each process.</li> <li>Write a program which uses MPI_Abort to exit gracefully unless there are 4 processes running.</li> </ul>"},{"location":"tutorials/message-passing-programming/02-point-to-point-communication/","title":"Point to Point Communication","text":""},{"location":"tutorials/message-passing-programming/02-point-to-point-communication/#description","title":"Description","text":"<p>This tutorial looks at how to send a message containing some data from one process to another.</p>"},{"location":"tutorials/message-passing-programming/02-point-to-point-communication/#prerequisites","title":"Prerequisites","text":"<p>Complete part one of this series if you haven't already.</p> <p>Now that we've got an idea on how to compile and run a trivial program using MPI, let's try and do something a bit more useful. One of the central concepts of message passing programming is of course passing messages. This means that we send some data from one process to another. We're going to look at a ping-pong program which sends a message back and forth between two processes. Let's start with the usual MPI boilerplate. Put this in a file called <code>ping_pong.c</code>.</p> <pre><code>#include &lt;mpi.h&gt;\n#include &lt;stdio.h&gt;\n\nint main(int argc, char** argv) {\n    MPI_Init(NULL, NULL);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &amp;size);\n\n    if (size != 2) {\n        printf(\"World size must be 2. Exiting...\\n\");\n        MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n\n    printf(\"Hello from %d\\n\", rank);\n    MPI_Finalize();\n}\n</code></pre> <p>Compile it with</p> <pre><code>$ module load mpi\n$ mpicc ping_pong.c -o ping_pong\n</code></pre> <p>and the following batch file can be used to run it on ENUCC</p> <pre><code>#!/bin/bash\n#SBATCH -o ping_pong.output\n#SBATCH -J \n#SBATCH --time=5\n#SBATCH --mem=128\n#SBATCH --ntasks=2\n\nmpirun -n 2 ping_pong\n</code></pre> <p>Now let's try and send a message from process 0 to process 1. Here we'll be sending an integer called ping_pong_count and later we'll increment it by one and send it back. For now we just send it from process 0 to process 1.</p> <pre><code>int main(int argc, char** argv) {\n    ...\n    int ping_pong_count;\n    int partner_rank = (rank + 1) % 2;\n    if (rank == 0) {\n        ping_pong_count = 0;\n        MPI_Send(&amp;ping_pong_count, 1, MPI_INT, partner_rank, 0, MPI_COMM_WORLD);\n        printf(\"Sent %d from process %d to process %d\\n\", ping_pong_count, rank, partner_rank);\n    } else {\n        MPI_Recv(&amp;ping_pong_count, 1, MPI_INT, partner_rank, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        printf(\"Received %d on process %d from process %d\\n\", ping_pong_count, rank, partner_rank);\n    }\n    ...\n}\n</code></pre> <p>If everything has gone right so far then when you run this you'll see the following in the output file</p> <pre><code>Sent 0 from process 0 to process 1\nReceived 0 on process 1 from process 0\n</code></pre> <p>So how did we send the message? The function signature for MPI_Send is</p> <pre><code>MPI_Send(\n    void *buf,\n    int count,\n    MPI_Datatype datatype,\n    int dest,\n    int tag,\n    MPI_Comm comm\n)\n</code></pre> <p>This is fairly complicated and it might take a while before you fully understand it. Here is a brief outline of the arguments</p> <ul> <li>buf - A pointer to the data to be sent. In our program we sent ping_pong_count.</li> <li>count - The size of the array to be sent. Since we are only sending one number, we set this to be 1.</li> <li>datatype - The type of data to be sent. Since we are sending an integer we used MPI_Int. See the list of MPI datatypes for more info.</li> <li>dest - The rank of the receiving process. Here we sent our message to <code>partner_rank</code>.</li> <li>tag - An identifier for this message. We don't use this, so set it to 0.</li> <li>comm - The communicator. We set this to <code>MPI_COMM_WORLD</code>.</li> </ul> <p>The receive message has an almost identical signature</p> <pre><code>MPI_Recv(\n    void *buf,\n    int count,\n    MPI_Datatype datatype,\n    int source,\n    int tag,\n    MPI_Comm comm,\n    MPI_status *status\n)\n</code></pre> <p>The only difference is that we also have the status argument. This can be used to determine whether an error occurred while sending the message but for now we set this to be <code>MPI_STATUS_IGNORE</code> since we aren't using the status.</p> <p>Hopefully it is quite clear now how we send an integer from one process to another. We call <code>MPI_Send</code> on the sending process and <code>MPI_Recv</code> on the receiving process.</p> <p>We now have all the knowledge we need to implement the ping pong program. Here it is in full.</p> <pre><code>#include &lt;mpi.h&gt;\n#include &lt;stdio.h&gt;\n#define PING_PONG_LIMIT 5\n\nint main(int argc, char** argv) {\n    MPI_Init(NULL, NULL);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &amp;size);\n\n    if (size != 2) {\n        printf(\"World size must be 2. Exiting...\\n\");\n        MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n\n    int ping_pong_count = 0;\n    int partner_rank = (rank + 1) % 2;\n\n    while(ping_pong_count &lt; PING_PONG_LIMIT) {\n        if (rank == ping_pong_count % 2) {\n            ping_pong_count ++;\n            MPI_Send(&amp;ping_pong_count, 1, MPI_INT, partner_rank, 0, MPI_COMM_WORLD);\n            printf(\"ping\\n\");\n        } else {\n            MPI_Recv(&amp;ping_pong_count, 1, MPI_INT, partner_rank, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            printf(\"pong\\n\");\n        }\n    }\n\n    MPI_Finalize();\n}\n</code></pre> <p>Running this, you'll see the following in the output file:</p> <pre><code>ping\npong\nping\npong\nping\npong\nping\npong\nping\npong\n</code></pre>"},{"location":"tutorials/message-passing-programming/02-point-to-point-communication/#exercises","title":"Exercises","text":"<ul> <li>Implement a ring-pong program. Send a counter in a ring around an arbitrary number of processes, incrementing the counter after each send. Terminate once every process has sent a message.</li> <li>Send a string from one process to another. Hint - a string is an array for chars.</li> </ul>"},{"location":"tutorials/message-passing-programming/03-using-probe/","title":"Using Probe","text":""},{"location":"tutorials/message-passing-programming/03-using-probe/#description","title":"Description","text":""},{"location":"tutorials/message-passing-programming/03-using-probe/#prerequisites","title":"Prerequisites","text":"<p>You should have already completed part two of this series.</p> <p>If we want to send a message where the length cannot be known at compile time then how do we handle receiving it? <code>MPI_Recv</code> requires that the amount of data be specified. The answer is that we need to use <code>MPI_Probe</code> to get the message length before we receive it. </p> <p>Here is a snippet illustrating how probe works. See the full program on github.</p> <pre><code>if (rank == 0) {\n    // Choose one of four animals\n    srand(time(NULL));\n    char animals[4][10] = {\"dog\", \"cat\", \"rat\", \"gorilla\"};\n    char animal[10];\n    int index = rand() % 4;\n    strcpy(animal, animals[index]);\n    printf(\"Sending %s of size %d\\n\", animal, strlen(animal));\n    MPI_Send(animal, strlen(animal), MPI_CHAR, 1, 0, MPI_COMM_WORLD);\n} else {\n    // Get status\n    MPI_Status stat;\n    MPI_Probe(0, 0, MPI_COMM_WORLD, &amp;stat);\n    int message_length;\n    MPI_Get_count(&amp;stat, MPI_CHAR, &amp;message_length);\n    printf(\"Receiving message of length %d\\n\", message_length);\n    // Receive message\n    char message[message_length];\n    MPI_Recv(&amp;message, message_length, MPI_CHAR, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    printf(\"Received %s from sender\\n\", message);\n}\n</code></pre> <p>For the code executed on rank 0 we just create the message as would normally be expected. On rank 1 we first create an <code>MPI_Status</code> called stat and use <code>MPI_Probe</code> to initialise it. We can then use the method <code>MPI_Get_count</code> to get the length of the message. Finally we receive the message as we normally would.</p>"},{"location":"tutorials/message-passing-programming/03-using-probe/#exercises","title":"Exercises","text":"<ul> <li>Create a program which sends user input from one process to another.</li> </ul>"},{"location":"tutorials/message-passing-programming/04-using-broadcast/","title":"Using Broadcast","text":""},{"location":"tutorials/message-passing-programming/04-using-broadcast/#description","title":"Description","text":""},{"location":"tutorials/message-passing-programming/04-using-broadcast/#prerequisites","title":"Prerequisites","text":"<p>You should have completed the other parts of this series.</p> <p>Broadcast is another useful operation which is frequently used. It is useful for synchronising values across all processes. Hopefully you're getting used to the syntax of MPI commands and this example should be enough to understand how broadcast works. As always you can check the relevant documentation and see the example on github.</p> <pre><code>#include &lt;mpi.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;time.h&gt;\n\nint main(int argc, char** argv) {\n    srand(time(NULL));\n\n    MPI_Init(NULL, NULL);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);\n    int i;\n    if (rank == 0) {\n        i = rand();\n        printf(\"sending %d to all processes\\n\", i);\n    }\n\n    MPI_Bcast(&amp;i, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    printf(\"%d\\n\", i);\n    MPI_Finalize();\n}\n</code></pre>"},{"location":"tutorials/message-passing-programming/05-using-scatter-and-gather/","title":"Using Scatter and Gather","text":""},{"location":"tutorials/message-passing-programming/05-using-scatter-and-gather/#description","title":"Description","text":""},{"location":"tutorials/message-passing-programming/05-using-scatter-and-gather/#prerequisites","title":"Prerequisites","text":"<p>A very common use for message passing programming is in parallelising loops. Often if we are looping over an array we want to distribute the array across all processes. In order to do this, we use the <code>MPI_Scatter</code> routine. </p> <p>Let's say we have some code which iterates over an array.</p> <pre><code>for (int i =0; i &lt; array_length; i++) {\n    printf(\"Array value is %d\\n\", array[i]);\n}\n</code></pre> <p>The equivalent, using <code>MPI_Scatter</code> would be</p> <pre><code>int subarray_length = array_length / world_size;\nint subarray[subarray_length];\nMPI_Scatter(\n    array,               // Scatter `array`\n    subarray_length,     // Put this number on each process\n    MPI_INT,             // We are sending integers\n    subarray,            // Receive to `subarray`\n    subarray_length,     // Receive this many\n    MPI_INT,             // integers.\n    0,                   // Send from process 0\n    MPI_COMM_WORLD       // On COMM_WORLD.\n);\n\nfor (int i = 0; i &lt; subarray_length; i++) {\n    printf(\"Array value is %d on rank %d\\n\", array[i], rank);\n}\n</code></pre> <p>This simply puts a chunk of the array on each of the processes. Note that it is important that the array is divisible by the number of processes. If this is not the case then you have to use <code>MPI_Scatterv</code> instead.</p> <p>There is an inverse process to <code>MPI_Scatter</code> which rather than send data out to all processes will instead gather data from each process onto a single process. Let's take monte-carlo integration as an example.</p> <p>Here is a function which will estimate the integral of sin(x) from 0 to pi.</p> <pre><code>double integrate_sin(int ntrials) {\n    double sum = 0;\n    for (int i = 0; i &lt; ntrials; i++) {\n        double rand_int = (double)(rand()) / (double)(RAND_MAX) * M_PI;\n        sum += sin(rand_int);\n    }\n    return sum * M_PI / ntrials;\n}\n</code></pre> <p>Since this is effectively an averaging process, we can get estimates of the integration on each process and then average all of the estimates to get a more accurate estimate. </p> <pre><code>// Get a different estimate on each process\ndouble sin_estimate = integrate_sin(1000000000);\n\ndouble *results;\nMPI_Barrier(MPI_COMM_WORLD);\nif (rank == 0) {\n    results = malloc(sizeof(double) * world_size);\n}\n\nMPI_Gather(\n    &amp;sin_estimate,  // Send `sin_estimate`\n    1,              // which is of length 1\n    MPI_DOUBLE,     // and is type double.\n    results,        // Receive messages to `results`.\n    1,              // Each message consists of 1\n    MPI_DOUBLE,     // double.\n    0,              // Gather to rank 0\n    MPI_COMM_WORLD  // on COMM_WORLD.\n);\n\nif (rank == 0) {\n    double aggregate_estimate = 0;\n    for (int i = 0; i &lt; world_size; i++) {\n        aggregate_estimate += results[i];\n    }\n    aggregate_estimate /= world_size;\n    printf(\"Aggregate estimate = %f \\n\", aggregate_estimate);\n}\n</code></pre> <p>Finally, it is quite common to use scatter and then gather together to parallelise a task. As an example, we'll look at calculating the sum of an array. Here is a simple function to sum an array of integers.</p> <pre><code>double sum_array(double *arr, int arr_size) {\n    double sum = 0;\n    for (int i = 0; i&lt;arr_size; i++) {\n        sum += arr[i];\n    }\n    return sum;\n}\n</code></pre> <p>To parallelise this we can distribute the array across all processes and calculate the sum of each subarray. We can then sum those results. We could use the <code>MPI_Gather</code> routine, but <code>MPI_Reduce</code> is better suited for the task here.</p> <pre><code>int main(int argc, char** argv) {\n    int rank, world_size;\n    mpi_setup(&amp;world_size, &amp;rank);\n\n    if (rank == 0) {\n        fill_array(array);\n    }\n\n    int elements_per_proc = ARRAY_SIZE/world_size;\n    int *sub_array = malloc(sizeof(int) * elements_per_proc);\n\n    MPI_Scatter(array, elements_per_proc, MPI_INT, sub_array, elements_per_proc, MPI_INT, 0, MPI_COMM_WORLD);\n\n    int sub_sum = sum_array(sub_array, elements_per_proc);\n\n    int *sub_sums;\n    if (rank==0) {\n        sub_sums = (int *)malloc(sizeof(int) * world_size);\n    }\n    int total_sum;\n    MPI_Reduce(\n        &amp;sub_sum,       // Send sub_sum\n        &amp;total_sum,     // Put the result in total_sum\n        1,              // One element from each proc\n        MPI_INT,        // of type int.\n        MPI_SUM,        // Sum the results.\n        0,              // Store the output on rank 0\n        MPI_COMM_WORLD  // on COMM_WORLD.\n    );\n\n    if (rank==0) {\n        printf(\"%d\\n\", total_sum);\n        free(sub_sums);\n    }\n    free(sub_array);\n    MPI_Finalize();\n}\n</code></pre> <p>One new concept from this is the use of <code>MPI_SUM</code> which is of type <code>MPI_Op</code>. There are a few other MPI_Ops which are useful when used with <code>MPI_Reduce</code>, particularly <code>MPI_MAX</code> and <code>MPI_MIN</code>. You can read a more complete discussion on the mpi tutorial site.</p>"},{"location":"tutorials/message-passing-programming/06-asynchronous-messages/","title":"Asynchronous methods","text":""},{"location":"useful-extras/bash-cheatsheet/","title":"Bash Cheatsheet","text":"<p>This page contains an overview of some of the most commonly used bash commands. Every bash command consists of a name which you type first, followed by a number of arguments. Some arguments start with <code>-</code> or <code>--</code> which are used to pass flags to the command which change its behaviour.</p>"},{"location":"useful-extras/bash-cheatsheet/#directory-management","title":"Directory management","text":""},{"location":"useful-extras/bash-cheatsheet/#ls","title":"<code>ls</code>","text":"<pre><code>ls\nls /path/to/directory\n</code></pre> <p>Lists the contents of the supplied directory. If no directory is given, it lists the contents of the current working directory.</p>"},{"location":"useful-extras/bash-cheatsheet/#pwd","title":"<code>pwd</code>","text":"<pre><code>pwd\n</code></pre> <p>Stands for 'print working directory'. Prints the working directory to the screen.</p>"},{"location":"useful-extras/bash-cheatsheet/#cd","title":"<code>cd</code>","text":"<pre><code>cd path/to/directory\n</code></pre> <p>Stands for change directory. Changes the working directory to the specified path. If no path is given it will change directory to the home directory.</p>"},{"location":"useful-extras/bash-cheatsheet/#mkdir","title":"<code>mkdir</code>","text":"<pre><code>mkdir directory_name\n</code></pre> <p>Stands for make directory. Creates a new directory called <code>directory_name</code>. Note that it is recommended that you do not use spaces in filenames.</p>"},{"location":"useful-extras/bash-cheatsheet/#mvdir","title":"<code>mvdir</code>","text":"<pre><code>mvdir directory_name new_directory_name\n</code></pre> <p>Stands for move directory. Renames <code>directory_name</code> to <code>new_directory_name</code>.</p>"},{"location":"useful-extras/bash-cheatsheet/#rmdir","title":"<code>rmdir</code>","text":"<pre><code>rmdir empty_directory\n</code></pre> <p>Stands for remove directory. Deletes an empty directory. Once it's deleted, there's no way to get it back and this command won't prompt you before deleting so use with caution.</p>"},{"location":"useful-extras/bash-cheatsheet/#working-with-files","title":"Working with files","text":""},{"location":"useful-extras/bash-cheatsheet/#touch","title":"<code>touch</code>","text":"<pre><code>touch filename\n</code></pre> <p>touch is often used to create a new empty file. If the file already exists then it will update the date modified time to the current time.</p>"},{"location":"useful-extras/bash-cheatsheet/#cp","title":"<code>cp</code>","text":"<pre><code>cp oldfile newfile\n</code></pre> <p>cp stands for copy. It will copy oldfile to newfile. Note that if newfile already exists then this command will overwrite it and it is impossible to get the overwritten file back.</p>"},{"location":"useful-extras/bash-cheatsheet/#mv","title":"<code>mv</code>","text":"<pre><code>mv oldfile newfile\n</code></pre> <p>mv stands for move. It is similar to copy, but will delete oldfile. Similarly, if newfile exists it will be permanently overwritten.</p>"},{"location":"useful-extras/bash-cheatsheet/#cat","title":"<code>cat</code>","text":"<pre><code>cat file1\ncat file1 file2 file3\n</code></pre> <p>cat stands for concatenate. If you give it the name of one file, it will print the contents to the terminal. If you give it multiple files it will concatenate the contents together and then print that to the terminal. This is quite useful for inspecting files from the command line.</p>"},{"location":"useful-extras/bash-cheatsheet/#more-advanced-operations","title":"More advanced operations","text":""},{"location":"useful-extras/bash-cheatsheet/#piping-","title":"piping - <code>|</code>","text":"<p>Piping allows you to use the output of one command as the input for another. For example, there is a program called wc, meaning word count. If we give it a file, it will print the number of newlines, the number of words and the number of bytes in the file.</p> <pre><code>$ wc file\n 3 4 24 file\n</code></pre> <p>Here there were 3 newlines, 4 words and 24 byes.</p> <p>We can use this to count the number of files and subdirectories in the current directory by piping the output of <code>ls</code> to <code>wc</code>.</p> <pre><code>$ ls | wc\n 5 5 44\n</code></pre>"},{"location":"useful-extras/bash-cheatsheet/#redirecting-to-a-file-and","title":"redirecting to a file - <code>&gt;</code> and <code>&gt;&gt;</code>","text":"<p>The <code>&gt;</code> and <code>&gt;&gt;</code> commands redirect the output of a command to a file. For example we can save the output of ls to a file.</p> <pre><code>$ ls &gt; ls_output_file\n$ cat ls_output_file\nbin\ncopy_projects\nls_output_file\nminiconda3\nprojects\ntest2\n</code></pre> <p><code>&gt;</code> creates a new file if it doesn't exist and overwrites a file that does exist. The specified file will only contain the output of the redirected command.</p> <p><code>&gt;&gt;</code> will append the output to the specified file.</p>"},{"location":"useful-extras/bash-cheatsheet/#wildcards-","title":"wildcards - <code>*</code>","text":"<p>You can use <code>*</code> to work with more than one file, substituting <code>*</code> for the changing parts of filenames. Here are some examples.</p> <pre><code>ls *A       # List all files ending in A\nls A*       # List all files starting with A\ncp *.txt textfiles/    # Copies all files ending in .txt to the textfiles directory\n</code></pre>"},{"location":"useful-extras/bash-cheatsheet/#others","title":"Others","text":""},{"location":"useful-extras/bash-cheatsheet/#echo","title":"<code>echo</code>","text":"<pre><code>$ echo hello\nhello\n</code></pre> <p>Prints any text after the echo command to the command line. Often used with <code>&gt;</code> or <code>&gt;&gt;</code> for adding text to files. For example</p> <pre><code>$ echo hello &gt; textfile\n$ cat textfile\nhello\n</code></pre>"},{"location":"useful-extras/bash-cheatsheet/#sudo","title":"<code>sudo</code>","text":"<p>Stands for superuser do. This is the linux equivalent to running as an administrator. Do not use this unless strictly necessary. You won't have sudo access on ENUCC.</p>"},{"location":"useful-extras/bash-cheatsheet/#_1","title":"<code>.</code>","text":"<p>Used to refer to the current directory.</p> <pre><code>ls ./\n</code></pre>"},{"location":"useful-extras/bash-cheatsheet/#_2","title":"<code>..</code>","text":"<p>Used to refer to the directory above</p> <pre><code>$ pwd\n/users/username/projects/directory\n$ cd ..\n$ pwd\n/users/username/projects\n</code></pre>"},{"location":"useful-extras/bash-cheatsheet/#_3","title":"<code>$?</code>","text":"<p>A variable which refers to the exit code of the last run command. Can be used with echo to check whether the last command ran successfully. In general an exit code of 0 is successful and anything else means the program exited with an error.</p> <pre><code>$ ls\n$ echo $?\n0\n$ ls /non/existent/directory\n$ echo $?\n2\n</code></pre>"},{"location":"useful-extras/bash-cheatsheet/#_4","title":"<code>!!</code>","text":"<p>Can be used to run the last command again.</p>"},{"location":"useful-extras/bash-cheatsheet/#devnull","title":"<code>/dev/null</code>","text":"<p>A psuedofile which is deleted immediately. Useful for getting rid of noisy output. For example if you want to run a program but don't want to see its output you can redirect it to /dev/null.</p> <pre><code>python noisyprogram.py &gt; /dev/null\n</code></pre>"},{"location":"useful-extras/git/","title":"Using git","text":""},{"location":"useful-extras/git/#description","title":"Description","text":"<p>This tutorial will outline the basics of using git.</p>"},{"location":"useful-extras/git/#prerequisites","title":"Prerequisites","text":"<p>None.</p> <p>Git is a general purpose tool for tracking changes in a filesystem and for collaboratively working with others. There are a few ways to use git, some more complicated than others. In this tutorial we'll first look at using git for a project on which you are the only person making changes. In this scenario git is useful for tracking changes and also for keeping a remote copy of your work as a backup. After we become comfortable working alone with git, we'll look at using git to collaborate with others.</p> <p>Before we get started I want to cover a few bits of terminology which are frequently used.</p> <ul> <li>git: git is the name of the tool used for version control.</li> <li>Repository: all of your project files, along with your git history and data.</li> <li>GitHub: a website which allows you to host a copy of your repository on their servers.</li> <li>Commit: a snapshot of your project files</li> </ul> <p>To get started you can follow the instructions on github to create a new repository. Once you've created a repository, which is stored on github's servers, you can clone the repository to have a copy on your own machine.</p> <pre><code>$ git clone git@github.com:SCEBE-Technicians/&lt;repository-name&gt;.git\n</code></pre> <p>You can then make changes or create new files in the repository. Once you are happy with the changes made, we will want to make a commit. You can think of a commit as a snapshot of your project. Committing your changes happens in two parts - first you add the changed files you want to be included in the commit to the staging area, then you create the commit from the files in the staging area.</p> <pre><code>$ git add file1 file2 file3\n$ git commit -m \"Create file2 and file3 and change file1\"\n</code></pre> <p>The changes will not be visible on the remote repository (on github) yet, because we've only made the changes locally. In order to sync the remote repository with our local repository, we have to push the changes. </p> <pre><code>$ git push\n</code></pre>"},{"location":"useful-extras/git/#working-with-others","title":"Working with others","text":"<p>So far, everything we've done has been on the main branch.</p>"},{"location":"useful-extras/git/#useful-resources","title":"Useful resources","text":"<ul> <li>If you are looking to get deep into what git can really do and how it works then I'd recommend reading pro git. It's freely available online and is one of the best resources there is.</li> </ul>"},{"location":"useful-extras/logging-in-with-putty/","title":"Logging in with PuTTY","text":"<p>To log in with putty, first install it via apps anywhere. When you launch the application it should look like so:</p> <p></p> <p>Into the input box labelled Host Name (or IP address) type <code>4XXXXXXX@login.enucc.napier.ac.uk</code>, replacing the X's with your staff or student number.</p> <p></p> <p>The first time you do this, you'll be presented with the following warning. Press accept.</p> <p></p> <p>You'll then be prompted to type in your password. Nothing will show on screen as you type, this is normal.</p> <p></p> <p>Press enter, and you're now connected to a bash shell on the login node.</p> <p></p>"},{"location":"useful-extras/mobaxterm/","title":"Using MobaXTerm","text":"<p>MobaXTerm is a feature rich terminal emulator which runs on windows. You can download it from the official website.</p> <p>To access ENUCC with MobaXTerm, first open the program.</p> <p></p> <p>In the top right press the session button.</p> <p></p> <p>Select SSH and fill out the host and username fields. For the Remote host you should type <code>login.enucc.napier.ac.uk</code>. Check the box which says \"Specify username\" then type in your staff or student number as the username.</p> <p>If you are accessing ENUCC from within the university then you can press OK and type in your password when prompted.</p> <p>If you are accessing from outside the university you'll need to use the SSH gateway. Navigate to the Network settings tab and click on the button which says \"SSH gateway (jump host).</p> <p></p> <p>For the Gateway host field put <code>gateway.napier.ac.uk</code>. For the username put your staff or student number. Press OK once those are filled out.</p> <p></p> <p>Once you're connected, you'll see a filetree on the left hand side of the screen. You can upload files from your local computer to ENUCC by dragging them into this box.</p> <p></p>"},{"location":"useful-extras/technical-specs/","title":"Technical Specs","text":"<p>ENUCC has three separate partitions named: node, himem and gpu.</p> Partition Number of Nodes Cpu RAM node 8 AMD EPYC 7513 32-Core Processor 512 GB himem 2 AMD EPYC 7763 64-Core Processor 2   TB gpu 4 AMD EPYC 7543 32-Core Processor 2   TB <p>Each gpu node has a two NVIDIA A100 80GB PCIe GPU's</p>"},{"location":"useful-extras/using-filezilla/","title":"Using FileZilla","text":"<p>Filezilla is a program which allows you to easily move files between your local machine and a remote server (ENUCC in this case). If you have a managed machine you can download FileZilla from AppsAnywhere. Otherwise you can download the FileZilla client from the website.</p> <p>When you open FileZilla you'll see the following screen.</p> <p></p> <p>At the top of the screen you'll see a few fields which you can fill in. For the Host you'll want to put <code>login.enucc.napier.ac.uk</code>. For username you'll need to put your university staff or student number, <code>4XXXXXXX</code>. For the password, you'll type in your usual password. Finally for the port you'll need to put <code>22</code>.</p> <p></p> <p>You will get a popup which asks whether you want FileZilla to remember your password. I personally would advise against it, but you can decide for yourself.</p> <p></p> <p>Once you're connected, you'll see your local filesystem in the two boxes on the left hand side and the ENUCC filesystem on the right hand side.</p> <p></p> <p>To download a file from ENUCC to your local machine, navigate to the file on ENUCC that you want to download. On the left hand side navigate to the location that you want the file to be downloaded to. In order to download the file, you simple right click the file and select download.</p> <p></p> <p>Uploading works in exactly the same way except that you select a file on your local machine on the left hand side. Right click the file and press upload.</p> <p></p>"},{"location":"useful-extras/using-filezilla/#uploading-through-the-ssh-gateway","title":"Uploading through the SSH Gateway","text":"<p>If you are not connected to a university network then uploading and downloading files can be a bit more complicated. Instructions to follow soon...</p>"}]}